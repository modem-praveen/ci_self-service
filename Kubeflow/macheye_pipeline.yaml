apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: macheye-pipeline-
  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.19, pipelines.kubeflow.org/pipeline_compilation_time: '2023-03-06T14:21:52.166181',
    pipelines.kubeflow.org/pipeline_spec: '{"inputs": [{"name": "Transcription_query",
      "type": "JsonObject"}, {"name": "agg_metric_query", "type": "JsonObject"}, {"name":
      "table_details_promotor", "type": "JsonObject"}, {"name": "greenplum_details",
      "type": "JsonObject"}, {"name": "table_details_keywords", "type": "JsonObject"},
      {"name": "table_details_topics", "type": "JsonObject"}, {"name": "table_details_sentiment",
      "type": "JsonObject"}, {"name": "table_details_aspect", "type": "JsonObject"},
      {"name": "table_details_last_modified", "type": "JsonObject"}], "name": "Macheye
      pipeline"}'}
  labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.19}
spec:
  entrypoint: macheye-pipeline
  templates:
  - name: fetch-data-mongo
    container:
      args: [--config, '{{inputs.parameters.query-maker-transformed_query_agg}}',
        --fetched-data, /tmp/outputs/fetched_data/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pymongo' 'boto3' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
        --quiet --no-warn-script-location 'pymongo' 'boto3' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def fetch_data_mongo(config, fetched_data_path, query_addon = None,
                             query_time = None):
            # necessary external library imports
            import boto3
            from botocore.exceptions import ClientError
            from pymongo import MongoClient
            from json import dump, loads
            from urllib import parse

            secret_name = config['secret_name']

            # Create a Secrets Manager client
            session = boto3.session.Session()
            client = session.client(service_name='secretsmanager', region_name='us-east-1')

            try:
                get_secret_value_response = client.get_secret_value(
                    SecretId=secret_name
                )
            except ClientError as e:
                # For a list of exceptions thrown, see
                # https://docs.aws.amazon.com/secretsmanager/latest/apireference/API_GetSecretValue.html
                raise e

            # Decrypts secret using the associated KMS key.
            secret = loads(get_secret_value_response['SecretString'])
            print(f'\nFetched all required secrets from aws secrets manager')

            # connection string varies by instance - etl, gbm, fm_app
            connection_string = "mongodb://" + secret['user'] + ":" + parse.quote_plus(secret['password']) + '@' + \
                                secret['host'] + ':' + secret['port']
            ssl = secret.get('ssl') in [True, 'True', 'true', '1', 1]
            mongo_client = MongoClient(connection_string, ssl=ssl)
            db = mongo_client.get_database(config['db_name'])
            collection = db.get_collection(config['collection_name'])

            # creating the query
            queries_sources = [config.get('query'), query_addon, query_time]
            query = [x for x in queries_sources if x is not None]
            if query:
                query = {'$and': query}
            else:
                query = None

            fields = dict(zip(config['fields'], [1] * len(config['fields'])))
            fields['_id'] = 0

            # pull all documents/records
            print("\nQuery : ", query)
            cursor = collection.find(query, fields).batch_size(config['batch_size'])
            print(f"\nPulling data from Db: {config['db_name']} & Collection: {config['collection_name']}")

            data = list()
            for record in cursor:
                data.append(record)

            print(f"\nTotal fetched records {len(data)}")

            # close mongo connection
            mongo_client.close()
            print(f"\nMongo Client Connection Closed")

            # writing fetched data to a file
            with open(fetched_data_path, 'w') as file:
                dump(data, file)
            file.close()

        import json
        import argparse
        _parser = argparse.ArgumentParser(prog='Fetch data mongo', description='')
        _parser.add_argument("--config", dest="config", type=json.loads, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--query-addon", dest="query_addon", type=json.loads, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--query-time", dest="query_time", type=json.loads, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--fetched-data", dest="fetched_data_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = fetch_data_mongo(**_parsed_args)
      image: python:3.11
      imagePullPolicy: IfNotPresent
    inputs:
      parameters:
      - {name: query-maker-transformed_query_agg}
    outputs:
      artifacts:
      - {name: fetch-data-mongo-fetched_data, path: /tmp/outputs/fetched_data/data}
    metadata:
      annotations: {pipelines.kubeflow.org/task_display_name: Fetching agg_metric,
        pipelines.kubeflow.org/component_spec: '{"implementation": {"container": {"args":
          ["--config", {"inputValue": "config"}, {"if": {"cond": {"isPresent": "query_addon"},
          "then": ["--query-addon", {"inputValue": "query_addon"}]}}, {"if": {"cond":
          {"isPresent": "query_time"}, "then": ["--query-time", {"inputValue": "query_time"}]}},
          "--fetched-data", {"outputPath": "fetched_data"}], "command": ["sh", "-c",
          "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          ''pymongo'' ''boto3'' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip
          install --quiet --no-warn-script-location ''pymongo'' ''boto3'' --user)
          && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\"
          > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef fetch_data_mongo(config, fetched_data_path, query_addon
          = None,\n                     query_time = None):\n    # necessary external
          library imports\n    import boto3\n    from botocore.exceptions import ClientError\n    from
          pymongo import MongoClient\n    from json import dump, loads\n    from urllib
          import parse\n\n    secret_name = config[''secret_name'']\n\n    # Create
          a Secrets Manager client\n    session = boto3.session.Session()\n    client
          = session.client(service_name=''secretsmanager'', region_name=''us-east-1'')\n\n    try:\n        get_secret_value_response
          = client.get_secret_value(\n            SecretId=secret_name\n        )\n    except
          ClientError as e:\n        # For a list of exceptions thrown, see\n        #
          https://docs.aws.amazon.com/secretsmanager/latest/apireference/API_GetSecretValue.html\n        raise
          e\n\n    # Decrypts secret using the associated KMS key.\n    secret = loads(get_secret_value_response[''SecretString''])\n    print(f''\\nFetched
          all required secrets from aws secrets manager'')\n\n    # connection string
          varies by instance - etl, gbm, fm_app\n    connection_string = \"mongodb://\"
          + secret[''user''] + \":\" + parse.quote_plus(secret[''password'']) + ''@''
          + \\\n                        secret[''host''] + '':'' + secret[''port'']\n    ssl
          = secret.get(''ssl'') in [True, ''True'', ''true'', ''1'', 1]\n    mongo_client
          = MongoClient(connection_string, ssl=ssl)\n    db = mongo_client.get_database(config[''db_name''])\n    collection
          = db.get_collection(config[''collection_name''])\n\n    # creating the query\n    queries_sources
          = [config.get(''query''), query_addon, query_time]\n    query = [x for x
          in queries_sources if x is not None]\n    if query:\n        query = {''$and'':
          query}\n    else:\n        query = None\n\n    fields = dict(zip(config[''fields''],
          [1] * len(config[''fields''])))\n    fields[''_id''] = 0\n\n    # pull all
          documents/records\n    print(\"\\nQuery : \", query)\n    cursor = collection.find(query,
          fields).batch_size(config[''batch_size''])\n    print(f\"\\nPulling data
          from Db: {config[''db_name'']} & Collection: {config[''collection_name'']}\")\n\n    data
          = list()\n    for record in cursor:\n        data.append(record)\n\n    print(f\"\\nTotal
          fetched records {len(data)}\")\n\n    # close mongo connection\n    mongo_client.close()\n    print(f\"\\nMongo
          Client Connection Closed\")\n\n    # writing fetched data to a file\n    with
          open(fetched_data_path, ''w'') as file:\n        dump(data, file)\n    file.close()\n\nimport
          json\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Fetch data
          mongo'', description='''')\n_parser.add_argument(\"--config\", dest=\"config\",
          type=json.loads, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--query-addon\",
          dest=\"query_addon\", type=json.loads, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--query-time\",
          dest=\"query_time\", type=json.loads, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--fetched-data\",
          dest=\"fetched_data_path\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = fetch_data_mongo(**_parsed_args)\n"], "image": "python:3.11"}}, "inputs":
          [{"name": "config", "type": "JsonObject"}, {"name": "query_addon", "optional":
          true, "type": "JsonObject"}, {"name": "query_time", "optional": true, "type":
          "JsonObject"}], "name": "Fetch data mongo", "outputs": [{"name": "fetched_data",
          "type": "JsonObject"}]}', pipelines.kubeflow.org/component_ref: '{"digest":
          "f53bc7c20e574139b25b4c42d6cb90aad1aa9c4e679baf38df528b34390b52fd", "url":
          "https://x-access-token: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@raw.githubusercontent.com/gnanarepo/kubeflow-utils/v1.2.2/mongo/fetch/component.yaml"}',
        pipelines.kubeflow.org/arguments.parameters: '{"config": "{{inputs.parameters.query-maker-transformed_query_agg}}"}',
        pipelines.kubeflow.org/max_cache_staleness: P0D}
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.19
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
  - name: fetch-data-mongo-2
    container:
      args: [--config, '{{inputs.parameters.query-maker-transformed_query_trans}}',
        --fetched-data, /tmp/outputs/fetched_data/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pymongo' 'boto3' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
        --quiet --no-warn-script-location 'pymongo' 'boto3' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def fetch_data_mongo(config, fetched_data_path, query_addon = None,
                             query_time = None):
            # necessary external library imports
            import boto3
            from botocore.exceptions import ClientError
            from pymongo import MongoClient
            from json import dump, loads
            from urllib import parse

            secret_name = config['secret_name']

            # Create a Secrets Manager client
            session = boto3.session.Session()
            client = session.client(service_name='secretsmanager', region_name='us-east-1')

            try:
                get_secret_value_response = client.get_secret_value(
                    SecretId=secret_name
                )
            except ClientError as e:
                # For a list of exceptions thrown, see
                # https://docs.aws.amazon.com/secretsmanager/latest/apireference/API_GetSecretValue.html
                raise e

            # Decrypts secret using the associated KMS key.
            secret = loads(get_secret_value_response['SecretString'])
            print(f'\nFetched all required secrets from aws secrets manager')

            # connection string varies by instance - etl, gbm, fm_app
            connection_string = "mongodb://" + secret['user'] + ":" + parse.quote_plus(secret['password']) + '@' + \
                                secret['host'] + ':' + secret['port']
            ssl = secret.get('ssl') in [True, 'True', 'true', '1', 1]
            mongo_client = MongoClient(connection_string, ssl=ssl)
            db = mongo_client.get_database(config['db_name'])
            collection = db.get_collection(config['collection_name'])

            # creating the query
            queries_sources = [config.get('query'), query_addon, query_time]
            query = [x for x in queries_sources if x is not None]
            if query:
                query = {'$and': query}
            else:
                query = None

            fields = dict(zip(config['fields'], [1] * len(config['fields'])))
            fields['_id'] = 0

            # pull all documents/records
            print("\nQuery : ", query)
            cursor = collection.find(query, fields).batch_size(config['batch_size'])
            print(f"\nPulling data from Db: {config['db_name']} & Collection: {config['collection_name']}")

            data = list()
            for record in cursor:
                data.append(record)

            print(f"\nTotal fetched records {len(data)}")

            # close mongo connection
            mongo_client.close()
            print(f"\nMongo Client Connection Closed")

            # writing fetched data to a file
            with open(fetched_data_path, 'w') as file:
                dump(data, file)
            file.close()

        import json
        import argparse
        _parser = argparse.ArgumentParser(prog='Fetch data mongo', description='')
        _parser.add_argument("--config", dest="config", type=json.loads, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--query-addon", dest="query_addon", type=json.loads, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--query-time", dest="query_time", type=json.loads, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--fetched-data", dest="fetched_data_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = fetch_data_mongo(**_parsed_args)
      image: python:3.11
      imagePullPolicy: IfNotPresent
    inputs:
      parameters:
      - {name: query-maker-transformed_query_trans}
    outputs:
      artifacts:
      - {name: fetch-data-mongo-2-fetched_data, path: /tmp/outputs/fetched_data/data}
    metadata:
      annotations: {pipelines.kubeflow.org/task_display_name: Fetching Transcription,
        pipelines.kubeflow.org/component_spec: '{"implementation": {"container": {"args":
          ["--config", {"inputValue": "config"}, {"if": {"cond": {"isPresent": "query_addon"},
          "then": ["--query-addon", {"inputValue": "query_addon"}]}}, {"if": {"cond":
          {"isPresent": "query_time"}, "then": ["--query-time", {"inputValue": "query_time"}]}},
          "--fetched-data", {"outputPath": "fetched_data"}], "command": ["sh", "-c",
          "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          ''pymongo'' ''boto3'' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip
          install --quiet --no-warn-script-location ''pymongo'' ''boto3'' --user)
          && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\"
          > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef fetch_data_mongo(config, fetched_data_path, query_addon
          = None,\n                     query_time = None):\n    # necessary external
          library imports\n    import boto3\n    from botocore.exceptions import ClientError\n    from
          pymongo import MongoClient\n    from json import dump, loads\n    from urllib
          import parse\n\n    secret_name = config[''secret_name'']\n\n    # Create
          a Secrets Manager client\n    session = boto3.session.Session()\n    client
          = session.client(service_name=''secretsmanager'', region_name=''us-east-1'')\n\n    try:\n        get_secret_value_response
          = client.get_secret_value(\n            SecretId=secret_name\n        )\n    except
          ClientError as e:\n        # For a list of exceptions thrown, see\n        #
          https://docs.aws.amazon.com/secretsmanager/latest/apireference/API_GetSecretValue.html\n        raise
          e\n\n    # Decrypts secret using the associated KMS key.\n    secret = loads(get_secret_value_response[''SecretString''])\n    print(f''\\nFetched
          all required secrets from aws secrets manager'')\n\n    # connection string
          varies by instance - etl, gbm, fm_app\n    connection_string = \"mongodb://\"
          + secret[''user''] + \":\" + parse.quote_plus(secret[''password'']) + ''@''
          + \\\n                        secret[''host''] + '':'' + secret[''port'']\n    ssl
          = secret.get(''ssl'') in [True, ''True'', ''true'', ''1'', 1]\n    mongo_client
          = MongoClient(connection_string, ssl=ssl)\n    db = mongo_client.get_database(config[''db_name''])\n    collection
          = db.get_collection(config[''collection_name''])\n\n    # creating the query\n    queries_sources
          = [config.get(''query''), query_addon, query_time]\n    query = [x for x
          in queries_sources if x is not None]\n    if query:\n        query = {''$and'':
          query}\n    else:\n        query = None\n\n    fields = dict(zip(config[''fields''],
          [1] * len(config[''fields''])))\n    fields[''_id''] = 0\n\n    # pull all
          documents/records\n    print(\"\\nQuery : \", query)\n    cursor = collection.find(query,
          fields).batch_size(config[''batch_size''])\n    print(f\"\\nPulling data
          from Db: {config[''db_name'']} & Collection: {config[''collection_name'']}\")\n\n    data
          = list()\n    for record in cursor:\n        data.append(record)\n\n    print(f\"\\nTotal
          fetched records {len(data)}\")\n\n    # close mongo connection\n    mongo_client.close()\n    print(f\"\\nMongo
          Client Connection Closed\")\n\n    # writing fetched data to a file\n    with
          open(fetched_data_path, ''w'') as file:\n        dump(data, file)\n    file.close()\n\nimport
          json\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Fetch data
          mongo'', description='''')\n_parser.add_argument(\"--config\", dest=\"config\",
          type=json.loads, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--query-addon\",
          dest=\"query_addon\", type=json.loads, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--query-time\",
          dest=\"query_time\", type=json.loads, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--fetched-data\",
          dest=\"fetched_data_path\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = fetch_data_mongo(**_parsed_args)\n"], "image": "python:3.11"}}, "inputs":
          [{"name": "config", "type": "JsonObject"}, {"name": "query_addon", "optional":
          true, "type": "JsonObject"}, {"name": "query_time", "optional": true, "type":
          "JsonObject"}], "name": "Fetch data mongo", "outputs": [{"name": "fetched_data",
          "type": "JsonObject"}]}', pipelines.kubeflow.org/component_ref: '{"digest":
          "f53bc7c20e574139b25b4c42d6cb90aad1aa9c4e679baf38df528b34390b52fd", "url":
          "https://x-access-token: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@raw.githubusercontent.com/gnanarepo/kubeflow-utils/v1.2.2/mongo/fetch/component.yaml"}',
        pipelines.kubeflow.org/arguments.parameters: '{"config": "{{inputs.parameters.query-maker-transformed_query_trans}}"}',
        pipelines.kubeflow.org/max_cache_staleness: P0D}
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.19
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
  - name: macheye-pipeline
    inputs:
      parameters:
      - {name: Transcription_query}
      - {name: agg_metric_query}
      - {name: greenplum_details}
      - {name: table_details_aspect}
      - {name: table_details_keywords}
      - {name: table_details_last_modified}
      - {name: table_details_promotor}
      - {name: table_details_sentiment}
      - {name: table_details_topics}
    dag:
      tasks:
      - name: fetch-data-mongo
        template: fetch-data-mongo
        dependencies: [query-maker]
        arguments:
          parameters:
          - {name: query-maker-transformed_query_agg, value: '{{tasks.query-maker.outputs.parameters.query-maker-transformed_query_agg}}'}
      - name: fetch-data-mongo-2
        template: fetch-data-mongo-2
        dependencies: [query-maker]
        arguments:
          parameters:
          - {name: query-maker-transformed_query_trans, value: '{{tasks.query-maker.outputs.parameters.query-maker-transformed_query_trans}}'}
      - name: prepare-aspects
        template: prepare-aspects
        dependencies: [fetch-data-mongo-2]
        arguments:
          artifacts:
          - {name: fetch-data-mongo-2-fetched_data, from: '{{tasks.fetch-data-mongo-2.outputs.artifacts.fetch-data-mongo-2-fetched_data}}'}
      - name: prepare-keywords
        template: prepare-keywords
        dependencies: [fetch-data-mongo-2]
        arguments:
          artifacts:
          - {name: fetch-data-mongo-2-fetched_data, from: '{{tasks.fetch-data-mongo-2.outputs.artifacts.fetch-data-mongo-2-fetched_data}}'}
      - name: prepare-keywords-2
        template: prepare-keywords-2
        dependencies: [fetch-data-mongo]
        arguments:
          artifacts:
          - {name: fetch-data-mongo-fetched_data, from: '{{tasks.fetch-data-mongo.outputs.artifacts.fetch-data-mongo-fetched_data}}'}
      - name: prepare-sentiment
        template: prepare-sentiment
        dependencies: [fetch-data-mongo-2]
        arguments:
          artifacts:
          - {name: fetch-data-mongo-2-fetched_data, from: '{{tasks.fetch-data-mongo-2.outputs.artifacts.fetch-data-mongo-2-fetched_data}}'}
      - name: prepare-topics
        template: prepare-topics
        dependencies: [fetch-data-mongo-2]
        arguments:
          artifacts:
          - {name: fetch-data-mongo-2-fetched_data, from: '{{tasks.fetch-data-mongo-2.outputs.artifacts.fetch-data-mongo-2-fetched_data}}'}
      - name: query-maker
        template: query-maker
        arguments:
          parameters:
          - {name: Transcription_query, value: '{{inputs.parameters.Transcription_query}}'}
          - {name: agg_metric_query, value: '{{inputs.parameters.agg_metric_query}}'}
          - {name: greenplum_details, value: '{{inputs.parameters.greenplum_details}}'}
          - {name: table_details_last_modified, value: '{{inputs.parameters.table_details_last_modified}}'}
      - name: write-data
        template: write-data
        dependencies: [prepare-keywords-2]
        arguments:
          parameters:
          - {name: greenplum_details, value: '{{inputs.parameters.greenplum_details}}'}
          - {name: prepare-keywords-2-promotor_dectractor_transformed, value: '{{tasks.prepare-keywords-2.outputs.parameters.prepare-keywords-2-promotor_dectractor_transformed}}'}
          - {name: table_details_promotor, value: '{{inputs.parameters.table_details_promotor}}'}
      - name: write-data-2
        template: write-data-2
        dependencies: [prepare-keywords]
        arguments:
          parameters:
          - {name: greenplum_details, value: '{{inputs.parameters.greenplum_details}}'}
          - {name: prepare-keywords-keyword_transformed, value: '{{tasks.prepare-keywords.outputs.parameters.prepare-keywords-keyword_transformed}}'}
          - {name: table_details_keywords, value: '{{inputs.parameters.table_details_keywords}}'}
      - name: write-data-3
        template: write-data-3
        dependencies: [prepare-topics]
        arguments:
          parameters:
          - {name: greenplum_details, value: '{{inputs.parameters.greenplum_details}}'}
          - {name: prepare-topics-topic_transformed, value: '{{tasks.prepare-topics.outputs.parameters.prepare-topics-topic_transformed}}'}
          - {name: table_details_topics, value: '{{inputs.parameters.table_details_topics}}'}
      - name: write-data-4
        template: write-data-4
        dependencies: [prepare-sentiment]
        arguments:
          parameters:
          - {name: greenplum_details, value: '{{inputs.parameters.greenplum_details}}'}
          - {name: prepare-sentiment-sentiment_transformed, value: '{{tasks.prepare-sentiment.outputs.parameters.prepare-sentiment-sentiment_transformed}}'}
          - {name: table_details_sentiment, value: '{{inputs.parameters.table_details_sentiment}}'}
      - name: write-data-5
        template: write-data-5
        dependencies: [prepare-aspects]
        arguments:
          parameters:
          - {name: greenplum_details, value: '{{inputs.parameters.greenplum_details}}'}
          - {name: prepare-aspects-aspect_transformed, value: '{{tasks.prepare-aspects.outputs.parameters.prepare-aspects-aspect_transformed}}'}
          - {name: table_details_aspect, value: '{{inputs.parameters.table_details_aspect}}'}
      - name: write-data-6
        template: write-data-6
        dependencies: [write-data, write-data-2, write-data-3, write-data-4]
        arguments:
          parameters:
          - {name: greenplum_details, value: '{{inputs.parameters.greenplum_details}}'}
          - {name: table_details_last_modified, value: '{{inputs.parameters.table_details_last_modified}}'}
  - name: prepare-aspects
    container:
      args: [--Transcription-data, /tmp/inputs/Transcription_data/data, --aspect-transformed,
        /tmp/outputs/aspect_transformed/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pandas' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
        --no-warn-script-location 'pandas' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def prepare_aspects(Transcription_data_path , aspect_transformed_path ):
            import json
            from json import load
            import pandas as pd

            null = "null"
            with open(Transcription_data_path, 'r') as file:
                aspect_data = load(file)
            file.close()
            data=aspect_data

            if data != []:
                rows = []
                for obj in data:
                    meeting_id = obj["object"]["meeting_id"]
                    deal_ids = obj["object"]["deal_ids"]
                    positive_aspects = []
                    negative_aspects = []
                    if "positive_aspects" in obj["object"]:
                        if obj["object"]["positive_aspects"] == "null" or obj["object"]["positive_aspects"] == None:
                            word_positions = None
                            word_count = None
                            aspect = None
                        else:
                            for aspect, values in obj["object"]["positive_aspects"].items():
                                word_positions = []
                                word_count = 0
                                for value in values:
                                    word_positions += value["wordPositions"]
                                    word_count += len(value["wordPositions"])
                                positive_aspects.append({"aspect": aspect, "word_count": word_count, "word_positions": word_positions})
                    if "negative_aspects" in obj["object"]:
                        if obj["object"]["negative_aspects"] == "null" or obj["object"]["negative_aspects"] == None:
                            word_positions = None
                            word_count = None
                            aspect = None
                        else:
                            for aspect, values in obj["object"]["negative_aspects"].items():
                                # loop through each value and extract the word positions and word count
                                word_positions = []
                                word_count = 0
                                for value in values:
                                    word_positions += value["wordPositions"]
                                    word_count += len(value["wordPositions"])
                                negative_aspects.append(
                                    {"aspect": aspect, "word_count": word_count, "word_positions": word_positions})
                    if not positive_aspects and not negative_aspects:
                        rows.append({"meeting_id": meeting_id, "deal_id": deal_ids, "aspect": None, "aspect_type": None,
                                     "aspect_count": None})
                    for aspect_dict in positive_aspects:
                        rows.append({"meeting_id": meeting_id, "deal_id": deal_ids, "aspect": aspect_dict["aspect"],
                                     "aspect_type": "Positive", "aspect_count": aspect_dict["word_count"]})
                    for aspect_dict in negative_aspects:
                        rows.append({"meeting_id": meeting_id, "deal_id": deal_ids, "aspect": aspect_dict["aspect"],
                                     "aspect_type": "Negative", "aspect_count": aspect_dict["word_count"]})

                df = pd.DataFrame(rows)
                df["deal_id"] = df['deal_id'].str[0]
                print("Number of records :",df.shape[0])
                print("Number of meeting_ids :",df["meeting_id"].count())
                print("Number of deal_ids :",df["deal_id"].count())
                print("Number of unique meeting_ids and deal_ids :",df["meeting_id"].nunique(),df["deal_id"].nunique())
                print("Number of aspect ",df["aspect"].count())
                print("Number of aspect ", df["aspect_type"].value_counts())
            else:
                print("Empty Data")
                df=pd.DataFrame()

            json_str = df.to_json(orient='records')
            json_str=json.loads(json_str)
            json.dump(json_str, open(aspect_transformed_path, 'w'))

        import argparse
        _parser = argparse.ArgumentParser(prog='Prepare aspects', description='')
        _parser.add_argument("--Transcription-data", dest="Transcription_data_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--aspect-transformed", dest="aspect_transformed_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = prepare_aspects(**_parsed_args)
      image: python:3.11
      imagePullPolicy: IfNotPresent
    inputs:
      artifacts:
      - {name: fetch-data-mongo-2-fetched_data, path: /tmp/inputs/Transcription_data/data}
    outputs:
      parameters:
      - name: prepare-aspects-aspect_transformed
        valueFrom: {path: /tmp/outputs/aspect_transformed/data}
      artifacts:
      - {name: prepare-aspects-aspect_transformed, path: /tmp/outputs/aspect_transformed/data}
    metadata:
      annotations: {pipelines.kubeflow.org/task_display_name: Aspects_transformation,
        pipelines.kubeflow.org/component_spec: '{"implementation": {"container": {"args":
          ["--Transcription-data", {"inputPath": "Transcription_data"}, "--aspect-transformed",
          {"outputPath": "aspect_transformed"}], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''pandas'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''pandas'' --user)
          && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\"
          > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef prepare_aspects(Transcription_data_path , aspect_transformed_path
          ):\n    import json\n    from json import load\n    import pandas as pd\n\n    null
          = \"null\"\n    with open(Transcription_data_path, ''r'') as file:\n        aspect_data
          = load(file)\n    file.close()\n    data=aspect_data\n\n    if data != []:\n        rows
          = []\n        for obj in data:\n            meeting_id = obj[\"object\"][\"meeting_id\"]\n            deal_ids
          = obj[\"object\"][\"deal_ids\"]\n            positive_aspects = []\n            negative_aspects
          = []\n            if \"positive_aspects\" in obj[\"object\"]:\n                if
          obj[\"object\"][\"positive_aspects\"] == \"null\" or obj[\"object\"][\"positive_aspects\"]
          == None:\n                    word_positions = None\n                    word_count
          = None\n                    aspect = None\n                else:\n                    for
          aspect, values in obj[\"object\"][\"positive_aspects\"].items():\n                        word_positions
          = []\n                        word_count = 0\n                        for
          value in values:\n                            word_positions += value[\"wordPositions\"]\n                            word_count
          += len(value[\"wordPositions\"])\n                        positive_aspects.append({\"aspect\":
          aspect, \"word_count\": word_count, \"word_positions\": word_positions})\n            if
          \"negative_aspects\" in obj[\"object\"]:\n                if obj[\"object\"][\"negative_aspects\"]
          == \"null\" or obj[\"object\"][\"negative_aspects\"] == None:\n                    word_positions
          = None\n                    word_count = None\n                    aspect
          = None\n                else:\n                    for aspect, values in
          obj[\"object\"][\"negative_aspects\"].items():\n                        #
          loop through each value and extract the word positions and word count\n                        word_positions
          = []\n                        word_count = 0\n                        for
          value in values:\n                            word_positions += value[\"wordPositions\"]\n                            word_count
          += len(value[\"wordPositions\"])\n                        negative_aspects.append(\n                            {\"aspect\":
          aspect, \"word_count\": word_count, \"word_positions\": word_positions})\n            if
          not positive_aspects and not negative_aspects:\n                rows.append({\"meeting_id\":
          meeting_id, \"deal_id\": deal_ids, \"aspect\": None, \"aspect_type\": None,\n                             \"aspect_count\":
          None})\n            for aspect_dict in positive_aspects:\n                rows.append({\"meeting_id\":
          meeting_id, \"deal_id\": deal_ids, \"aspect\": aspect_dict[\"aspect\"],\n                             \"aspect_type\":
          \"Positive\", \"aspect_count\": aspect_dict[\"word_count\"]})\n            for
          aspect_dict in negative_aspects:\n                rows.append({\"meeting_id\":
          meeting_id, \"deal_id\": deal_ids, \"aspect\": aspect_dict[\"aspect\"],\n                             \"aspect_type\":
          \"Negative\", \"aspect_count\": aspect_dict[\"word_count\"]})\n\n        df
          = pd.DataFrame(rows)\n        df[\"deal_id\"] = df[''deal_id''].str[0]\n        print(\"Number
          of records :\",df.shape[0])\n        print(\"Number of meeting_ids :\",df[\"meeting_id\"].count())\n        print(\"Number
          of deal_ids :\",df[\"deal_id\"].count())\n        print(\"Number of unique
          meeting_ids and deal_ids :\",df[\"meeting_id\"].nunique(),df[\"deal_id\"].nunique())\n        print(\"Number
          of aspect \",df[\"aspect\"].count())\n        print(\"Number of aspect \",
          df[\"aspect_type\"].value_counts())\n    else:\n        print(\"Empty Data\")\n        df=pd.DataFrame()\n\n    json_str
          = df.to_json(orient=''records'')\n    json_str=json.loads(json_str)\n    json.dump(json_str,
          open(aspect_transformed_path, ''w''))\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Prepare
          aspects'', description='''')\n_parser.add_argument(\"--Transcription-data\",
          dest=\"Transcription_data_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--aspect-transformed\",
          dest=\"aspect_transformed_path\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = prepare_aspects(**_parsed_args)\n"], "image": "python:3.11"}}, "inputs":
          [{"name": "Transcription_data", "type": "JsonObject"}], "name": "Prepare
          aspects", "outputs": [{"name": "aspect_transformed", "type": "JsonObject"}]}',
        pipelines.kubeflow.org/component_ref: '{"digest": "12810862b0c33f130b336958cb2c5933d34c559637fc0a8ede7e3047764d0566",
          "url": "Aspects/component.yaml"}', pipelines.kubeflow.org/max_cache_staleness: P0D}
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.19
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
  - name: prepare-keywords
    container:
      args: [--Transcription-data, /tmp/inputs/Transcription_data/data, --keyword-transformed,
        /tmp/outputs/keyword_transformed/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pandas' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
        --no-warn-script-location 'pandas' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def prepare_keywords(Transcription_data_path , keyword_transformed_path ):
            import json
            from json import load
            import pandas as pd

            null = "null"
            # load text data
            with open(Transcription_data_path, 'r') as file:
                opps_data = load(file)
            file.close()
            data=opps_data

            df = pd.DataFrame(columns=["meeting_id", "deal_id", "keyword", "keyword_count"])
            if data != []:
                for item in data:
                    meeting_id = item["object"]["meeting_id"]
                    deal_ids = item["object"]["deal_ids"]
                    for keyword in item["object"]["keywords"]:
                        count = 0
                        for word_dict in item["object"]["keywords"][keyword]:
                            count = count + len(word_dict["wordPositions"])
                        df = df.append({"meeting_id": meeting_id, "deal_id": deal_ids, "keyword": keyword, "keyword_count": count},
                                       ignore_index=True)
                df["deal_id"] = df['deal_id'].str[0]

                print("Number of records :",df.shape[0])
                print("Number of meeting_ids :",df["meeting_id"].count())
                print("Number of deal_ids :",df["deal_id"].count())
                print("Number of unique meeting_ids and deal_ids :",df["meeting_id"].nunique(),df["deal_id"].nunique())
                print("Number of keywords ",df["keyword"].count())
            else:
                print("Empty Data")

            json_str = df.to_json(orient='records')
            json_str=json.loads(json_str)
            json.dump(json_str, open(keyword_transformed_path, 'w'))

        import argparse
        _parser = argparse.ArgumentParser(prog='Prepare keywords', description='')
        _parser.add_argument("--Transcription-data", dest="Transcription_data_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--keyword-transformed", dest="keyword_transformed_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = prepare_keywords(**_parsed_args)
      image: python:3.11
      imagePullPolicy: IfNotPresent
    inputs:
      artifacts:
      - {name: fetch-data-mongo-2-fetched_data, path: /tmp/inputs/Transcription_data/data}
    outputs:
      parameters:
      - name: prepare-keywords-keyword_transformed
        valueFrom: {path: /tmp/outputs/keyword_transformed/data}
      artifacts:
      - {name: prepare-keywords-keyword_transformed, path: /tmp/outputs/keyword_transformed/data}
    metadata:
      annotations: {pipelines.kubeflow.org/task_display_name: Keywords_transformation,
        pipelines.kubeflow.org/component_spec: '{"implementation": {"container": {"args":
          ["--Transcription-data", {"inputPath": "Transcription_data"}, "--keyword-transformed",
          {"outputPath": "keyword_transformed"}], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''pandas'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''pandas'' --user)
          && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\"
          > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef prepare_keywords(Transcription_data_path , keyword_transformed_path
          ):\n    import json\n    from json import load\n    import pandas as pd\n\n    null
          = \"null\"\n    # load text data\n    with open(Transcription_data_path,
          ''r'') as file:\n        opps_data = load(file)\n    file.close()\n    data=opps_data\n\n    df
          = pd.DataFrame(columns=[\"meeting_id\", \"deal_id\", \"keyword\", \"keyword_count\"])\n    if
          data != []:\n        for item in data:\n            meeting_id = item[\"object\"][\"meeting_id\"]\n            deal_ids
          = item[\"object\"][\"deal_ids\"]\n            for keyword in item[\"object\"][\"keywords\"]:\n                count
          = 0\n                for word_dict in item[\"object\"][\"keywords\"][keyword]:\n                    count
          = count + len(word_dict[\"wordPositions\"])\n                df = df.append({\"meeting_id\":
          meeting_id, \"deal_id\": deal_ids, \"keyword\": keyword, \"keyword_count\":
          count},\n                               ignore_index=True)\n        df[\"deal_id\"]
          = df[''deal_id''].str[0]\n\n        print(\"Number of records :\",df.shape[0])\n        print(\"Number
          of meeting_ids :\",df[\"meeting_id\"].count())\n        print(\"Number of
          deal_ids :\",df[\"deal_id\"].count())\n        print(\"Number of unique
          meeting_ids and deal_ids :\",df[\"meeting_id\"].nunique(),df[\"deal_id\"].nunique())\n        print(\"Number
          of keywords \",df[\"keyword\"].count())\n    else:\n        print(\"Empty
          Data\")\n\n    json_str = df.to_json(orient=''records'')\n    json_str=json.loads(json_str)\n    json.dump(json_str,
          open(keyword_transformed_path, ''w''))\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Prepare
          keywords'', description='''')\n_parser.add_argument(\"--Transcription-data\",
          dest=\"Transcription_data_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--keyword-transformed\",
          dest=\"keyword_transformed_path\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = prepare_keywords(**_parsed_args)\n"], "image": "python:3.11"}}, "inputs":
          [{"name": "Transcription_data", "type": "JsonObject"}], "name": "Prepare
          keywords", "outputs": [{"name": "keyword_transformed", "type": "JsonObject"}]}',
        pipelines.kubeflow.org/component_ref: '{"digest": "45be30c2b8454b02d7bd5a03ea1d5f10d2f44cabef2322206aa67f69461367a1",
          "url": "Keywords/component.yaml"}', pipelines.kubeflow.org/max_cache_staleness: P0D}
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.19
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
  - name: prepare-keywords-2
    container:
      args: [--agg-metric-data, /tmp/inputs/agg_metric_data/data, --promotor-dectractor-transformed,
        /tmp/outputs/promotor_dectractor_transformed/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pandas' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
        --no-warn-script-location 'pandas' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def prepare_keywords(agg_metric_data_path , promotor_dectractor_transformed_path ):
            import json
            from json import load
            import pandas as pd

            null = "null"
            # load text data
            with open(agg_metric_data_path, 'r') as file:
                promotor_dectractor_data = load(file)
            file.close()
            data=promotor_dectractor_data
            # Create an empty list to store the data
            output_data = []
            if data != []:
                for obj in data:
                    meeting_id = obj['object']['meeting_id']
                    deal_ids = obj['object']['deal_ids']
                    # Iterate over each speaker in the user map
                    for speaker, speaker_data in obj['object']['user_map'].items():
                        speaker_name = speaker_data['name']
                        is_buyer = []
                        if "is_buyer" in list(speaker_data.keys()):
                            is_buyer.append(speaker_data['is_buyer'])
                        else:
                            is_buyer.append(None)
                        speaker_sentiment = obj['object']['speaker_sentiment_time_count'][speaker]
                        positive = speaker_sentiment['POSITIVE']
                        negative = speaker_sentiment['NEGATIVE']
                        neutral = speaker_sentiment['NEUTRAL']
                        score = (positive - negative) / (positive + negative + neutral) if positive + negative + neutral > 0 else 0
                        if score >= 0.5 :
                            Promotor_or_Detractor="Promoter"
                        elif score<0.5:
                            Promotor_or_Detractor="Detractor"
                        # Append the speaker data to the output list
                        output_data.append({
                            'meeting_id': meeting_id,
                            'deal_id': deal_ids,
                            'speaker_tag': speaker,
                            'speaker_sentiment_score': score,
                            'speaker_name': speaker_name,
                            'promoter_or_detractor':Promotor_or_Detractor,
                            'is_buyer': str(is_buyer[0])
                        })
                # Create a DataFrame from the output data
                df = pd.DataFrame(output_data, columns=['meeting_id', 'deal_id', 'speaker_tag', 'speaker_sentiment_score', 'speaker_name',"promoter_or_detractor","is_buyer"])
                df["deal_id"] = df['deal_id'].str[0]
                print("Number of records :",df.shape[0])
                print("Number of meeting_ids :",df["meeting_id"].count())
                print("Number of deal_ids :",df["deal_id"].count())
                print("Number of unique meeting_ids and deal_ids :",df["meeting_id"].nunique(),df["deal_id"].nunique())
                print("Number of aspect ",df["speaker_tag"].count())
                print("Number of aspect ", df["is_buyer"].count())

            else:
                print("Empty Data")
                df=pd.DataFrame()
            json_str = df.to_json(orient='records')
            json_str=json.loads(json_str)
            json.dump(json_str, open(promotor_dectractor_transformed_path, 'w'))

        import argparse
        _parser = argparse.ArgumentParser(prog='Prepare keywords', description='')
        _parser.add_argument("--agg-metric-data", dest="agg_metric_data_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--promotor-dectractor-transformed", dest="promotor_dectractor_transformed_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = prepare_keywords(**_parsed_args)
      image: python:3.11
      resources:
        requests: {memory: 8G, cpu: '2'}
      imagePullPolicy: IfNotPresent
    inputs:
      artifacts:
      - {name: fetch-data-mongo-fetched_data, path: /tmp/inputs/agg_metric_data/data}
    outputs:
      parameters:
      - name: prepare-keywords-2-promotor_dectractor_transformed
        valueFrom: {path: /tmp/outputs/promotor_dectractor_transformed/data}
      artifacts:
      - {name: prepare-keywords-2-promotor_dectractor_transformed, path: /tmp/outputs/promotor_dectractor_transformed/data}
    metadata:
      annotations: {pipelines.kubeflow.org/task_display_name: promoter_or_detractor_transformation,
        pipelines.kubeflow.org/component_spec: '{"implementation": {"container": {"args":
          ["--agg-metric-data", {"inputPath": "agg_metric_data"}, "--promotor-dectractor-transformed",
          {"outputPath": "promotor_dectractor_transformed"}], "command": ["sh", "-c",
          "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          ''pandas'' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
          --no-warn-script-location ''pandas'' --user) && \"$0\" \"$@\"", "sh", "-ec",
          "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef prepare_keywords(agg_metric_data_path , promotor_dectractor_transformed_path
          ):\n    import json\n    from json import load\n    import pandas as pd\n\n    null
          = \"null\"\n    # load text data\n    with open(agg_metric_data_path, ''r'')
          as file:\n        promotor_dectractor_data = load(file)\n    file.close()\n    data=promotor_dectractor_data\n    #
          Create an empty list to store the data\n    output_data = []\n    if data
          != []:\n        for obj in data:\n            meeting_id = obj[''object''][''meeting_id'']\n            deal_ids
          = obj[''object''][''deal_ids'']\n            # Iterate over each speaker
          in the user map\n            for speaker, speaker_data in obj[''object''][''user_map''].items():\n                speaker_name
          = speaker_data[''name'']\n                is_buyer = []\n                if
          \"is_buyer\" in list(speaker_data.keys()):\n                    is_buyer.append(speaker_data[''is_buyer''])\n                else:\n                    is_buyer.append(None)\n                speaker_sentiment
          = obj[''object''][''speaker_sentiment_time_count''][speaker]\n                positive
          = speaker_sentiment[''POSITIVE'']\n                negative = speaker_sentiment[''NEGATIVE'']\n                neutral
          = speaker_sentiment[''NEUTRAL'']\n                score = (positive - negative)
          / (positive + negative + neutral) if positive + negative + neutral > 0 else
          0\n                if score >= 0.5 :\n                    Promotor_or_Detractor=\"Promoter\"\n                elif
          score<0.5:\n                    Promotor_or_Detractor=\"Detractor\"\n                #
          Append the speaker data to the output list\n                output_data.append({\n                    ''meeting_id'':
          meeting_id,\n                    ''deal_id'': deal_ids,\n                    ''speaker_tag'':
          speaker,\n                    ''speaker_sentiment_score'': score,\n                    ''speaker_name'':
          speaker_name,\n                    ''promoter_or_detractor'':Promotor_or_Detractor,\n                    ''is_buyer'':
          str(is_buyer[0])\n                })\n        # Create a DataFrame from
          the output data\n        df = pd.DataFrame(output_data, columns=[''meeting_id'',
          ''deal_id'', ''speaker_tag'', ''speaker_sentiment_score'', ''speaker_name'',\"promoter_or_detractor\",\"is_buyer\"])\n        df[\"deal_id\"]
          = df[''deal_id''].str[0]\n        print(\"Number of records :\",df.shape[0])\n        print(\"Number
          of meeting_ids :\",df[\"meeting_id\"].count())\n        print(\"Number of
          deal_ids :\",df[\"deal_id\"].count())\n        print(\"Number of unique
          meeting_ids and deal_ids :\",df[\"meeting_id\"].nunique(),df[\"deal_id\"].nunique())\n        print(\"Number
          of aspect \",df[\"speaker_tag\"].count())\n        print(\"Number of aspect
          \", df[\"is_buyer\"].count())\n\n    else:\n        print(\"Empty Data\")\n        df=pd.DataFrame()\n    json_str
          = df.to_json(orient=''records'')\n    json_str=json.loads(json_str)\n    json.dump(json_str,
          open(promotor_dectractor_transformed_path, ''w''))\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Prepare keywords'', description='''')\n_parser.add_argument(\"--agg-metric-data\",
          dest=\"agg_metric_data_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--promotor-dectractor-transformed\",
          dest=\"promotor_dectractor_transformed_path\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = prepare_keywords(**_parsed_args)\n"], "image": "python:3.11"}}, "inputs":
          [{"name": "agg_metric_data", "type": "JsonObject"}], "name": "Prepare keywords",
          "outputs": [{"name": "promotor_dectractor_transformed", "type": "JsonObject"}]}',
        pipelines.kubeflow.org/component_ref: '{"digest": "6cc39cf5d0b28d5a7bf5d54c2b1c9983aef582b5e22c8e3a2275d730b1421ef1",
          "url": "Promoter_or_Detractor/component.yaml"}', pipelines.kubeflow.org/max_cache_staleness: P0D}
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.19
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
  - name: prepare-sentiment
    container:
      args: [--Transcription-data, /tmp/inputs/Transcription_data/data, --sentiment-transformed,
        /tmp/outputs/sentiment_transformed/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pandas' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
        --no-warn-script-location 'pandas' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def prepare_sentiment(Transcription_data_path , sentiment_transformed_path ):
            import json
            from json import load
            import pandas as pd
            import datetime
            import math

            null = "null"
            # load text data
            with open(Transcription_data_path, 'r') as file:
                sentiment_data = load(file)
            file.close()
            data = sentiment_data

            df = pd.DataFrame(columns=["meeting_id", "deal_id", "meeting_date", "sentiment_score", "sentiment_sategory"])

            if data != []:
                for meeting in data:
                    meeting_id = meeting["object"]["meeting_id"]
                    deal_id = meeting["object"]["deal_ids"]
                    date = meeting["object"]["meeting_date"]
                    try:
                        sentiment_score = meeting["object"]["buyer_call_sentiment"]
                        if sentiment_score is not None:
                            if sentiment_score >= 0.6:
                                sentiment_category = 'Strongly Positive'
                            elif 0.2 <= sentiment_score < 0.6:
                                sentiment_category = 'Positive'
                            elif -0.2 < sentiment_score < 0.2:
                                sentiment_category = 'Neutral'
                            elif -0.6 < sentiment_score <= -0.2:
                                sentiment_category = 'Negative'
                            else:
                                sentiment_category = 'Strongly Negative'
                        else:
                            sentiment_category = None
                    except:
                        sentiment_score = None
                        sentiment_category = None
                    df = df.append({"meeting_id": meeting_id,
                                    "deal_id": deal_id,
                                    "meeting_date": date,
                                    "sentiment_score": sentiment_score,
                                    "sentiment_category": sentiment_category
                                    }, ignore_index=True)

                # define lambda function to convert Unix timestamp to date string
                unix_to_date = lambda x: '' if math.isnan(x) else datetime.datetime.utcfromtimestamp(x).strftime('%d-%m-%Y')
                df["meeting_date"] = pd.to_datetime(df["meeting_date"], unit='ms', errors='coerce')
                df["deal_id"] = df['deal_id'].str[0]

                print("Number of records :",df.shape[0])
                print("Number of meeting_ids :",df["meeting_id"].count())
                print("Number of deal_ids :",df["deal_id"].count())
                print("Number of unique meeting_ids and deal_id :",df["meeting_id"].nunique(),df["deal_id"].nunique())
                print("Number of Date ",df["meeting_date"].count())
            else:
                print("Empty Data")

            json_str = df.to_json(orient='records')
            json_str=json.loads(json_str)
            json.dump(json_str, open(sentiment_transformed_path, 'w'))

        import argparse
        _parser = argparse.ArgumentParser(prog='Prepare sentiment', description='')
        _parser.add_argument("--Transcription-data", dest="Transcription_data_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--sentiment-transformed", dest="sentiment_transformed_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = prepare_sentiment(**_parsed_args)
      image: python:3.11
      imagePullPolicy: IfNotPresent
    inputs:
      artifacts:
      - {name: fetch-data-mongo-2-fetched_data, path: /tmp/inputs/Transcription_data/data}
    outputs:
      parameters:
      - name: prepare-sentiment-sentiment_transformed
        valueFrom: {path: /tmp/outputs/sentiment_transformed/data}
      artifacts:
      - {name: prepare-sentiment-sentiment_transformed, path: /tmp/outputs/sentiment_transformed/data}
    metadata:
      annotations: {pipelines.kubeflow.org/task_display_name: Sentiment_transformation,
        pipelines.kubeflow.org/component_spec: '{"implementation": {"container": {"args":
          ["--Transcription-data", {"inputPath": "Transcription_data"}, "--sentiment-transformed",
          {"outputPath": "sentiment_transformed"}], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''pandas'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''pandas'' --user)
          && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\"
          > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef prepare_sentiment(Transcription_data_path , sentiment_transformed_path
          ):\n    import json\n    from json import load\n    import pandas as pd\n    import
          datetime\n    import math\n\n    null = \"null\"\n    # load text data\n    with
          open(Transcription_data_path, ''r'') as file:\n        sentiment_data =
          load(file)\n    file.close()\n    data = sentiment_data\n\n    df = pd.DataFrame(columns=[\"meeting_id\",
          \"deal_id\", \"meeting_date\", \"sentiment_score\", \"sentiment_sategory\"])\n\n    if
          data != []:\n        for meeting in data:\n            meeting_id = meeting[\"object\"][\"meeting_id\"]\n            deal_id
          = meeting[\"object\"][\"deal_ids\"]\n            date = meeting[\"object\"][\"meeting_date\"]\n            try:\n                sentiment_score
          = meeting[\"object\"][\"buyer_call_sentiment\"]\n                if sentiment_score
          is not None:\n                    if sentiment_score >= 0.6:\n                        sentiment_category
          = ''Strongly Positive''\n                    elif 0.2 <= sentiment_score
          < 0.6:\n                        sentiment_category = ''Positive''\n                    elif
          -0.2 < sentiment_score < 0.2:\n                        sentiment_category
          = ''Neutral''\n                    elif -0.6 < sentiment_score <= -0.2:\n                        sentiment_category
          = ''Negative''\n                    else:\n                        sentiment_category
          = ''Strongly Negative''\n                else:\n                    sentiment_category
          = None\n            except:\n                sentiment_score = None\n                sentiment_category
          = None\n            df = df.append({\"meeting_id\": meeting_id,\n                            \"deal_id\":
          deal_id,\n                            \"meeting_date\": date,\n                            \"sentiment_score\":
          sentiment_score,\n                            \"sentiment_category\": sentiment_category\n                            },
          ignore_index=True)\n\n        # define lambda function to convert Unix timestamp
          to date string\n        unix_to_date = lambda x: '''' if math.isnan(x) else
          datetime.datetime.utcfromtimestamp(x).strftime(''%d-%m-%Y'')\n        df[\"meeting_date\"]
          = pd.to_datetime(df[\"meeting_date\"], unit=''ms'', errors=''coerce'')\n        df[\"deal_id\"]
          = df[''deal_id''].str[0]\n\n        print(\"Number of records :\",df.shape[0])\n        print(\"Number
          of meeting_ids :\",df[\"meeting_id\"].count())\n        print(\"Number of
          deal_ids :\",df[\"deal_id\"].count())\n        print(\"Number of unique
          meeting_ids and deal_id :\",df[\"meeting_id\"].nunique(),df[\"deal_id\"].nunique())\n        print(\"Number
          of Date \",df[\"meeting_date\"].count())\n    else:\n        print(\"Empty
          Data\")\n\n    json_str = df.to_json(orient=''records'')\n    json_str=json.loads(json_str)\n    json.dump(json_str,
          open(sentiment_transformed_path, ''w''))\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Prepare
          sentiment'', description='''')\n_parser.add_argument(\"--Transcription-data\",
          dest=\"Transcription_data_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--sentiment-transformed\",
          dest=\"sentiment_transformed_path\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = prepare_sentiment(**_parsed_args)\n"], "image": "python:3.11"}}, "inputs":
          [{"name": "Transcription_data", "type": "JsonObject"}], "name": "Prepare
          sentiment", "outputs": [{"name": "sentiment_transformed", "type": "JsonObject"}]}',
        pipelines.kubeflow.org/component_ref: '{"digest": "98a5f5d2d51b4bc5b80850ab6f92984a70dd216dff1fc01268c010c55234e89d",
          "url": "Sentiment/component.yaml"}', pipelines.kubeflow.org/max_cache_staleness: P0D}
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.19
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
  - name: prepare-topics
    container:
      args: [--Transcription-data, /tmp/inputs/Transcription_data/data, --topic-transformed,
        /tmp/outputs/topic_transformed/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pandas' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
        --no-warn-script-location 'pandas' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def prepare_topics(Transcription_data_path , topic_transformed_path ):
            import json
            from json import load
            import pandas as pd

            null="null"
            # load text data
            print('debug log')
            with open(Transcription_data_path, 'r') as file:
                data = load(file)
            file.close()
            # data=topic_data

            # Create an empty dataframe with the desired columns
            df = pd.DataFrame(columns=["meeting_id", "deal_id", "topic_name", "sub_topic_name", "sub_topic_count", "duration"])
            print("debug log")
            if data != []:
                for meeting in data:
                    meeting_id = meeting["object"]["meeting_id"]
                    deal_id = meeting["object"]["deal_ids"]
                    if meeting["object"]["topics"] == "null" or meeting["object"]["topics"] == None:
                        df = df.append({"meeting_id": meeting_id,
                                        "deal_id": deal_id,
                                        "topic_name": None,
                                        "sub_topic_name": None,
                                        "sub_topic_count": None,
                                        "duration": None}, ignore_index=True)

                    else:
                        for topic_name, topic_data in meeting["object"]["topics"].items():
                            for sub_topic in topic_data["sub_topics"]:
                                sub_topic_name = sub_topic["sub_topic"]
                                sub_topic_count = sub_topic["sub_topic_rate"]
                                duration = sub_topic["sub_topic_duration"]
                                df = df.append({"meeting_id": meeting_id,
                                                "deal_id": deal_id,  # No deal ID is provided in the data
                                                "topic_name": topic_name,
                                                "sub_topic_name": sub_topic_name,
                                                "sub_topic_count": sub_topic_count,
                                                "duration": duration}, ignore_index=True)
                df["deal_id"]=df['deal_id'].str[0]

                print("Number of records :",df.shape[0])
                print("Number of meeting_ids :",df["meeting_id"].count())
                print("Number of deal_ids :",df["deal_id"].count())
                print("Number of unique meeting_ids and deal_ids :",df["meeting_id"].nunique(),df["deal_id"].nunique())
                print("Number of aspect ",df["topic_name"].count())
            else:
                print("Empty Data")

            json_str = df.to_json(orient='records')
            json_str=json.loads(json_str)
            json.dump(json_str, open(topic_transformed_path, 'w'))

        import argparse
        _parser = argparse.ArgumentParser(prog='Prepare topics', description='')
        _parser.add_argument("--Transcription-data", dest="Transcription_data_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--topic-transformed", dest="topic_transformed_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = prepare_topics(**_parsed_args)
      image: python:3.11
      imagePullPolicy: IfNotPresent
    inputs:
      artifacts:
      - {name: fetch-data-mongo-2-fetched_data, path: /tmp/inputs/Transcription_data/data}
    outputs:
      parameters:
      - name: prepare-topics-topic_transformed
        valueFrom: {path: /tmp/outputs/topic_transformed/data}
      artifacts:
      - {name: prepare-topics-topic_transformed, path: /tmp/outputs/topic_transformed/data}
    metadata:
      annotations: {pipelines.kubeflow.org/task_display_name: Topic_transformation,
        pipelines.kubeflow.org/component_spec: '{"implementation": {"container": {"args":
          ["--Transcription-data", {"inputPath": "Transcription_data"}, "--topic-transformed",
          {"outputPath": "topic_transformed"}], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''pandas'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''pandas'' --user)
          && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\"
          > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef prepare_topics(Transcription_data_path , topic_transformed_path
          ):\n    import json\n    from json import load\n    import pandas as pd\n\n    null=\"null\"\n    #
          load text data\n    print(''debug log'')\n    with open(Transcription_data_path,
          ''r'') as file:\n        data = load(file)\n    file.close()\n    # data=topic_data\n\n    #
          Create an empty dataframe with the desired columns\n    df = pd.DataFrame(columns=[\"meeting_id\",
          \"deal_id\", \"topic_name\", \"sub_topic_name\", \"sub_topic_count\", \"duration\"])\n    print(\"debug
          log\")\n    if data != []:\n        for meeting in data:\n            meeting_id
          = meeting[\"object\"][\"meeting_id\"]\n            deal_id = meeting[\"object\"][\"deal_ids\"]\n            if
          meeting[\"object\"][\"topics\"] == \"null\" or meeting[\"object\"][\"topics\"]
          == None:\n                df = df.append({\"meeting_id\": meeting_id,\n                                \"deal_id\":
          deal_id,\n                                \"topic_name\": None,\n                                \"sub_topic_name\":
          None,\n                                \"sub_topic_count\": None,\n                                \"duration\":
          None}, ignore_index=True)\n\n            else:\n                for topic_name,
          topic_data in meeting[\"object\"][\"topics\"].items():\n                    for
          sub_topic in topic_data[\"sub_topics\"]:\n                        sub_topic_name
          = sub_topic[\"sub_topic\"]\n                        sub_topic_count = sub_topic[\"sub_topic_rate\"]\n                        duration
          = sub_topic[\"sub_topic_duration\"]\n                        df = df.append({\"meeting_id\":
          meeting_id,\n                                        \"deal_id\": deal_id,  #
          No deal ID is provided in the data\n                                        \"topic_name\":
          topic_name,\n                                        \"sub_topic_name\":
          sub_topic_name,\n                                        \"sub_topic_count\":
          sub_topic_count,\n                                        \"duration\":
          duration}, ignore_index=True)\n        df[\"deal_id\"]=df[''deal_id''].str[0]\n\n        print(\"Number
          of records :\",df.shape[0])\n        print(\"Number of meeting_ids :\",df[\"meeting_id\"].count())\n        print(\"Number
          of deal_ids :\",df[\"deal_id\"].count())\n        print(\"Number of unique
          meeting_ids and deal_ids :\",df[\"meeting_id\"].nunique(),df[\"deal_id\"].nunique())\n        print(\"Number
          of aspect \",df[\"topic_name\"].count())\n    else:\n        print(\"Empty
          Data\")\n\n    json_str = df.to_json(orient=''records'')\n    json_str=json.loads(json_str)\n    json.dump(json_str,
          open(topic_transformed_path, ''w''))\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Prepare
          topics'', description='''')\n_parser.add_argument(\"--Transcription-data\",
          dest=\"Transcription_data_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--topic-transformed\",
          dest=\"topic_transformed_path\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = prepare_topics(**_parsed_args)\n"], "image": "python:3.11"}}, "inputs":
          [{"name": "Transcription_data", "type": "JsonObject"}], "name": "Prepare
          topics", "outputs": [{"name": "topic_transformed", "type": "JsonObject"}]}',
        pipelines.kubeflow.org/component_ref: '{"digest": "cd3e6a8c514c0b6a66a2cea9163d79c7b4cb0c3cd388ea44ce6022b2efb39723",
          "url": "Topics/component.yaml"}', pipelines.kubeflow.org/max_cache_staleness: P0D}
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.19
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
  - name: query-maker
    container:
      args: [--table-details, '{{inputs.parameters.table_details_last_modified}}',
        --greenplum-details, '{{inputs.parameters.greenplum_details}}', --table-query-trans,
        '{{inputs.parameters.Transcription_query}}', --table-query-agg, '{{inputs.parameters.agg_metric_query}}',
        --transformed-query-trans, /tmp/outputs/transformed_query_trans/data, --transformed-query-agg,
        /tmp/outputs/transformed_query_agg/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pandas' 'psycopg2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
        --quiet --no-warn-script-location 'pandas' 'psycopg2' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def query_maker(table_details ,greenplum_details,table_query_trans,table_query_agg,transformed_query_trans,transformed_query_agg):
            import time
            import json
            from json import load
            import pandas as pd
            import psycopg2

            print("correct pipeline")
            conn = psycopg2.connect(
                host=greenplum_details["host"],
                database=greenplum_details["database"],
                user=greenplum_details["user_name"],
                password=greenplum_details["password"],
                port=greenplum_details["port"]
            )

            conn.autocommit = True
            cur = conn.cursor()
            table_name=table_details["table_name"]
            schema=table_details["schema"]
            column_name = schema.strip('()').split(' ')[0]
            schema_type= schema.strip('()').split(' ')[1]
            print(schema,column_name)
            try:
                cur.execute('''
                     CREATE TABLE {} ({} {});
                 '''.format(table_name,column_name,schema_type))

                print(table_name,column_name,schema)
                cur.execute('''INSERT INTO {} ({}) VALUES (0);'''.format(table_name,column_name))
                print("success")
            except:
                print("Table Exist")

            cur.execute("SELECT {} FROM {}".format(column_name,table_name))
            rows = cur.fetchall()
            for row in rows:
                print(row)
                value = int(row[0])  # or int(output[0])

            if table_details["fetch"] == "All":
                last_modified= 0
            else:
                last_modified = value

            print(last_modified)
            table_query_trans["query"] = {column_name: {"$gt":0}}
            table_query_agg["query"] = {column_name: {"$gt":0}}
            json.dump(table_query_trans, open(transformed_query_trans, 'w'))
            json.dump(table_query_agg, open(transformed_query_agg, 'w'))

        import json
        import argparse
        _parser = argparse.ArgumentParser(prog='Query maker', description='')
        _parser.add_argument("--table-details", dest="table_details", type=json.loads, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--greenplum-details", dest="greenplum_details", type=json.loads, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--table-query-trans", dest="table_query_trans", type=json.loads, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--table-query-agg", dest="table_query_agg", type=json.loads, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--transformed-query-trans", dest="transformed_query_trans", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--transformed-query-agg", dest="transformed_query_agg", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = query_maker(**_parsed_args)
      image: python:3.11
      imagePullPolicy: IfNotPresent
    inputs:
      parameters:
      - {name: Transcription_query}
      - {name: agg_metric_query}
      - {name: greenplum_details}
      - {name: table_details_last_modified}
    outputs:
      parameters:
      - name: query-maker-transformed_query_agg
        valueFrom: {path: /tmp/outputs/transformed_query_agg/data}
      - name: query-maker-transformed_query_trans
        valueFrom: {path: /tmp/outputs/transformed_query_trans/data}
      artifacts:
      - {name: query-maker-transformed_query_agg, path: /tmp/outputs/transformed_query_agg/data}
      - {name: query-maker-transformed_query_trans, path: /tmp/outputs/transformed_query_trans/data}
    metadata:
      annotations: {pipelines.kubeflow.org/task_display_name: Modify_Config, pipelines.kubeflow.org/component_spec: '{"implementation":
          {"container": {"args": ["--table-details", {"inputValue": "table_details"},
          "--greenplum-details", {"inputValue": "greenplum_details"}, "--table-query-trans",
          {"inputValue": "table_query_trans"}, "--table-query-agg", {"inputValue":
          "table_query_agg"}, "--transformed-query-trans", {"outputPath": "transformed_query_trans"},
          "--transformed-query-agg", {"outputPath": "transformed_query_agg"}], "command":
          ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
          --no-warn-script-location ''pandas'' ''psycopg2'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''pandas'' ''psycopg2''
          --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef query_maker(table_details ,greenplum_details,table_query_trans,table_query_agg,transformed_query_trans,transformed_query_agg):\n    import
          time\n    import json\n    from json import load\n    import pandas as pd\n    import
          psycopg2\n\n    print(\"correct pipeline\")\n    conn = psycopg2.connect(\n        host=greenplum_details[\"host\"],\n        database=greenplum_details[\"database\"],\n        user=greenplum_details[\"user_name\"],\n        password=greenplum_details[\"password\"],\n        port=greenplum_details[\"port\"]\n    )\n\n    conn.autocommit
          = True\n    cur = conn.cursor()\n    table_name=table_details[\"table_name\"]\n    schema=table_details[\"schema\"]\n    column_name
          = schema.strip(''()'').split('' '')[0]\n    schema_type= schema.strip(''()'').split(''
          '')[1]\n    print(schema,column_name)\n    try:\n        cur.execute(''''''\n             CREATE
          TABLE {} ({} {});\n         ''''''.format(table_name,column_name,schema_type))\n\n        print(table_name,column_name,schema)\n        cur.execute(''''''INSERT
          INTO {} ({}) VALUES (0);''''''.format(table_name,column_name))\n        print(\"success\")\n    except:\n        print(\"Table
          Exist\")\n\n    cur.execute(\"SELECT {} FROM {}\".format(column_name,table_name))\n    rows
          = cur.fetchall()\n    for row in rows:\n        print(row)\n        value
          = int(row[0])  # or int(output[0])\n\n    if table_details[\"fetch\"] ==
          \"All\":\n        last_modified= 0\n    else:\n        last_modified = value\n\n    print(last_modified)\n    table_query_trans[\"query\"]
          = {column_name: {\"$gt\":0}}\n    table_query_agg[\"query\"] = {column_name:
          {\"$gt\":0}}\n    json.dump(table_query_trans, open(transformed_query_trans,
          ''w''))\n    json.dump(table_query_agg, open(transformed_query_agg, ''w''))\n\nimport
          json\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Query maker'',
          description='''')\n_parser.add_argument(\"--table-details\", dest=\"table_details\",
          type=json.loads, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--greenplum-details\",
          dest=\"greenplum_details\", type=json.loads, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--table-query-trans\",
          dest=\"table_query_trans\", type=json.loads, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--table-query-agg\",
          dest=\"table_query_agg\", type=json.loads, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--transformed-query-trans\",
          dest=\"transformed_query_trans\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--transformed-query-agg\",
          dest=\"transformed_query_agg\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = query_maker(**_parsed_args)\n"], "image": "python:3.11"}}, "inputs": [{"name":
          "table_details", "type": "JsonObject"}, {"name": "greenplum_details", "type":
          "JsonObject"}, {"name": "table_query_trans", "type": "JsonObject"}, {"name":
          "table_query_agg", "type": "JsonObject"}], "name": "Query maker", "outputs":
          [{"name": "transformed_query_trans", "type": "JsonObject"}, {"name": "transformed_query_agg",
          "type": "JsonObject"}]}', pipelines.kubeflow.org/component_ref: '{"digest":
          "a927df6cada97300f52e798f6610afc7298da75c5e1d62b243ccc7f83ed548c1", "url":
          "Query_Maker/component.yaml"}', pipelines.kubeflow.org/arguments.parameters: '{"greenplum_details":
          "{{inputs.parameters.greenplum_details}}", "table_details": "{{inputs.parameters.table_details_last_modified}}",
          "table_query_agg": "{{inputs.parameters.agg_metric_query}}", "table_query_trans":
          "{{inputs.parameters.Transcription_query}}"}', pipelines.kubeflow.org/max_cache_staleness: P0D}
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.19
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
  - name: write-data
    container:
      args: [--greenplum-details, '{{inputs.parameters.greenplum_details}}', --table-details,
        '{{inputs.parameters.table_details_promotor}}', --greenplum-input-data-path,
        '{{inputs.parameters.prepare-keywords-2-promotor_dectractor_transformed}}']
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'psycopg2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
        --no-warn-script-location 'psycopg2' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def write_data(greenplum_details,table_details,greenplum_input_data_path = None ):
            import psycopg2
            import json
            from json import load
            import time
            # load text data
            print(greenplum_input_data_path)
            if greenplum_input_data_path != None and greenplum_input_data_path != [] :
                data = greenplum_input_data_path
            elif greenplum_input_data_path == None and greenplum_input_data_path != []:
                print(greenplum_input_data_path)
                print("writing last modified time to db")
                epoch_ts=int(time.time() * 1000)
                data = [{table_details["schema"].split(" ")[0][1:]: epoch_ts}]
            elif greenplum_input_data_path == []:
                data=greenplum_input_data_path

            if data != []:
                # Connect to Greenplum
                conn = psycopg2.connect(
                    host=greenplum_details["host"],
                    database=greenplum_details["database"],
                    user=greenplum_details["user_name"],
                    password=greenplum_details["password"],
                    port=greenplum_details["port"]
                )
                # Set autocommit to True to start a new transaction
                conn.autocommit = True
                cur = conn.cursor()

                mode=table_details["mode"]
                table_name=table_details["table_name"]
                schema=table_details["schema"]
                primary_key=table_details["primary_key"].split(',')

                # def bulk_load(data, table_name, schema, cur):
                if mode == "bulk_mode":
                    cur.execute(f"DROP TABLE IF EXISTS {table_name}")
                    cur.execute(f"""
                            CREATE TABLE {table_name} {schema};""")

                    for d in data:
                        keys=tuple(d.keys())
                        keys = str(keys)
                        value = tuple(d.values())
                        value_format = []
                        for cou in range(len(value)):
                            value_format.append('%s')

                        keys = keys.replace("'", "")
                        value_format = str(tuple(value_format))
                        value_format = value_format.replace("'", "")
                        print(keys,value_format,table_name)
                        if len(tuple(d.keys())) == 1 :
                            keys=keys.replace(",","")
                            value_format=value_format.replace(",","")
                        print(keys, value_format, table_name)
                        cur.execute(f"""
                            INSERT INTO {table_name} {keys}
                            VALUES {value_format};
                        """, value)
                    conn.commit()  # commit the transaction
                    conn.close()  # Close the connection
                else:
                    try:
                        cur.execute(f"""
                                CREATE TABLE {table_name} {schema};""")
                    except:
                        pass
                    for d in data:
                        pk_values = tuple(d[col] for col in primary_key)
                        try:
                            cur.execute(
                                f"SELECT EXISTS (SELECT 1 FROM {table_name} WHERE {' AND '.join([f'{col}=%s' for col in primary_key])})",
                                pk_values)
                            exists = cur.fetchone()[0]
                            if exists:
                                # print(f"Record with primary key {pk_values} already exists, updating...")
                                # Update the record
                                update_cols = [f"{col}=%s" for col in d.keys() if col not in primary_key]
                                update_values = tuple(d[col] for col in d.keys() if col not in primary_key)
                                update_query = f"UPDATE {table_name} SET {', '.join(update_cols)} WHERE {' AND '.join([f'{col}=%s' for col in primary_key])}"
                                cur.execute(update_query, update_values + pk_values)
                            else:
                                # print(f"Record with primary key {pk_values} does not exist, inserting...")
                                # Insert the record
                                insert_cols = list(primary_key) + [col for col in d.keys() if col not in primary_key]
                                insert_values = tuple(d[col] for col in insert_cols)
                                insert_query = f"INSERT INTO {table_name} ({', '.join(insert_cols)}) VALUES ({', '.join(['%s' for _ in insert_cols])})"
                                cur.execute(insert_query, insert_values)
                            conn.commit()
                        except Exception as e:
                            conn.rollback()

                    conn.commit()
                    conn.close()
            else:
                print("Data fetched is empty. So no records written to data base")

        import json
        import argparse
        _parser = argparse.ArgumentParser(prog='Write data', description='')
        _parser.add_argument("--greenplum-details", dest="greenplum_details", type=json.loads, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--table-details", dest="table_details", type=json.loads, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--greenplum-input-data-path", dest="greenplum_input_data_path", type=json.loads, required=False, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = write_data(**_parsed_args)
      image: python:3.11
      imagePullPolicy: IfNotPresent
    inputs:
      parameters:
      - {name: greenplum_details}
      - {name: prepare-keywords-2-promotor_dectractor_transformed}
      - {name: table_details_promotor}
    metadata:
      annotations: {pipelines.kubeflow.org/task_display_name: promoter_or_detractor_to_greenplum,
        pipelines.kubeflow.org/component_spec: '{"implementation": {"container": {"args":
          ["--greenplum-details", {"inputValue": "greenplum_details"}, "--table-details",
          {"inputValue": "table_details"}, {"if": {"cond": {"isPresent": "greenplum_input_data_path"},
          "then": ["--greenplum-input-data-path", {"inputValue": "greenplum_input_data_path"}]}}],
          "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip
          install --quiet --no-warn-script-location ''psycopg2'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''psycopg2'' --user)
          && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\"
          > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def write_data(greenplum_details,table_details,greenplum_input_data_path
          = None ):\n    import psycopg2\n    import json\n    from json import load\n    import
          time\n    # load text data\n    print(greenplum_input_data_path)\n    if
          greenplum_input_data_path != None and greenplum_input_data_path != [] :\n        data
          = greenplum_input_data_path\n    elif greenplum_input_data_path == None
          and greenplum_input_data_path != []:\n        print(greenplum_input_data_path)\n        print(\"writing
          last modified time to db\")\n        epoch_ts=int(time.time() * 1000)\n        data
          = [{table_details[\"schema\"].split(\" \")[0][1:]: epoch_ts}]\n    elif
          greenplum_input_data_path == []:\n        data=greenplum_input_data_path\n\n    if
          data != []:\n        # Connect to Greenplum\n        conn = psycopg2.connect(\n            host=greenplum_details[\"host\"],\n            database=greenplum_details[\"database\"],\n            user=greenplum_details[\"user_name\"],\n            password=greenplum_details[\"password\"],\n            port=greenplum_details[\"port\"]\n        )\n        #
          Set autocommit to True to start a new transaction\n        conn.autocommit
          = True\n        cur = conn.cursor()\n\n        mode=table_details[\"mode\"]\n        table_name=table_details[\"table_name\"]\n        schema=table_details[\"schema\"]\n        primary_key=table_details[\"primary_key\"].split('','')\n\n        #
          def bulk_load(data, table_name, schema, cur):\n        if mode == \"bulk_mode\":\n            cur.execute(f\"DROP
          TABLE IF EXISTS {table_name}\")\n            cur.execute(f\"\"\"\n                    CREATE
          TABLE {table_name} {schema};\"\"\")\n\n            for d in data:\n                keys=tuple(d.keys())\n                keys
          = str(keys)\n                value = tuple(d.values())\n                value_format
          = []\n                for cou in range(len(value)):\n                    value_format.append(''%s'')\n\n                keys
          = keys.replace(\"''\", \"\")\n                value_format = str(tuple(value_format))\n                value_format
          = value_format.replace(\"''\", \"\")\n                print(keys,value_format,table_name)\n                if
          len(tuple(d.keys())) == 1 :\n                    keys=keys.replace(\",\",\"\")\n                    value_format=value_format.replace(\",\",\"\")\n                print(keys,
          value_format, table_name)\n                cur.execute(f\"\"\"\n                    INSERT
          INTO {table_name} {keys}\n                    VALUES {value_format};\n                \"\"\",
          value)\n            conn.commit()  # commit the transaction\n            conn.close()  #
          Close the connection\n        else:\n            try:\n                cur.execute(f\"\"\"\n                        CREATE
          TABLE {table_name} {schema};\"\"\")\n            except:\n                pass\n            for
          d in data:\n                pk_values = tuple(d[col] for col in primary_key)\n                try:\n                    cur.execute(\n                        f\"SELECT
          EXISTS (SELECT 1 FROM {table_name} WHERE {'' AND ''.join([f''{col}=%s''
          for col in primary_key])})\",\n                        pk_values)\n                    exists
          = cur.fetchone()[0]\n                    if exists:\n                        #
          print(f\"Record with primary key {pk_values} already exists, updating...\")\n                        #
          Update the record\n                        update_cols = [f\"{col}=%s\"
          for col in d.keys() if col not in primary_key]\n                        update_values
          = tuple(d[col] for col in d.keys() if col not in primary_key)\n                        update_query
          = f\"UPDATE {table_name} SET {'', ''.join(update_cols)} WHERE {'' AND ''.join([f''{col}=%s''
          for col in primary_key])}\"\n                        cur.execute(update_query,
          update_values + pk_values)\n                    else:\n                        #
          print(f\"Record with primary key {pk_values} does not exist, inserting...\")\n                        #
          Insert the record\n                        insert_cols = list(primary_key)
          + [col for col in d.keys() if col not in primary_key]\n                        insert_values
          = tuple(d[col] for col in insert_cols)\n                        insert_query
          = f\"INSERT INTO {table_name} ({'', ''.join(insert_cols)}) VALUES ({'',
          ''.join([''%s'' for _ in insert_cols])})\"\n                        cur.execute(insert_query,
          insert_values)\n                    conn.commit()\n                except
          Exception as e:\n                    conn.rollback()\n\n            conn.commit()\n            conn.close()\n    else:\n        print(\"Data
          fetched is empty. So no records written to data base\")\n\nimport json\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Write data'', description='''')\n_parser.add_argument(\"--greenplum-details\",
          dest=\"greenplum_details\", type=json.loads, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--table-details\",
          dest=\"table_details\", type=json.loads, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--greenplum-input-data-path\",
          dest=\"greenplum_input_data_path\", type=json.loads, required=False, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = write_data(**_parsed_args)\n"],
          "image": "python:3.11"}}, "inputs": [{"name": "greenplum_details", "type":
          "JsonObject"}, {"name": "table_details", "type": "JsonObject"}, {"name":
          "greenplum_input_data_path", "optional": true, "type": "JsonObject"}], "name":
          "Write data"}', pipelines.kubeflow.org/component_ref: '{"digest": "cc5f610af5956a30ae335930d9a85d35002182c9eac1fd670d4278d19f1a16f3",
          "url": "Greenplum/write/component.yaml"}', pipelines.kubeflow.org/arguments.parameters: '{"greenplum_details":
          "{{inputs.parameters.greenplum_details}}", "greenplum_input_data_path":
          "{{inputs.parameters.prepare-keywords-2-promotor_dectractor_transformed}}",
          "table_details": "{{inputs.parameters.table_details_promotor}}"}', pipelines.kubeflow.org/max_cache_staleness: P0D}
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.19
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
  - name: write-data-2
    container:
      args: [--greenplum-details, '{{inputs.parameters.greenplum_details}}', --table-details,
        '{{inputs.parameters.table_details_keywords}}', --greenplum-input-data-path,
        '{{inputs.parameters.prepare-keywords-keyword_transformed}}']
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'psycopg2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
        --no-warn-script-location 'psycopg2' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def write_data(greenplum_details,table_details,greenplum_input_data_path = None ):
            import psycopg2
            import json
            from json import load
            import time
            # load text data
            print(greenplum_input_data_path)
            if greenplum_input_data_path != None and greenplum_input_data_path != [] :
                data = greenplum_input_data_path
            elif greenplum_input_data_path == None and greenplum_input_data_path != []:
                print(greenplum_input_data_path)
                print("writing last modified time to db")
                epoch_ts=int(time.time() * 1000)
                data = [{table_details["schema"].split(" ")[0][1:]: epoch_ts}]
            elif greenplum_input_data_path == []:
                data=greenplum_input_data_path

            if data != []:
                # Connect to Greenplum
                conn = psycopg2.connect(
                    host=greenplum_details["host"],
                    database=greenplum_details["database"],
                    user=greenplum_details["user_name"],
                    password=greenplum_details["password"],
                    port=greenplum_details["port"]
                )
                # Set autocommit to True to start a new transaction
                conn.autocommit = True
                cur = conn.cursor()

                mode=table_details["mode"]
                table_name=table_details["table_name"]
                schema=table_details["schema"]
                primary_key=table_details["primary_key"].split(',')

                # def bulk_load(data, table_name, schema, cur):
                if mode == "bulk_mode":
                    cur.execute(f"DROP TABLE IF EXISTS {table_name}")
                    cur.execute(f"""
                            CREATE TABLE {table_name} {schema};""")

                    for d in data:
                        keys=tuple(d.keys())
                        keys = str(keys)
                        value = tuple(d.values())
                        value_format = []
                        for cou in range(len(value)):
                            value_format.append('%s')

                        keys = keys.replace("'", "")
                        value_format = str(tuple(value_format))
                        value_format = value_format.replace("'", "")
                        print(keys,value_format,table_name)
                        if len(tuple(d.keys())) == 1 :
                            keys=keys.replace(",","")
                            value_format=value_format.replace(",","")
                        print(keys, value_format, table_name)
                        cur.execute(f"""
                            INSERT INTO {table_name} {keys}
                            VALUES {value_format};
                        """, value)
                    conn.commit()  # commit the transaction
                    conn.close()  # Close the connection
                else:
                    try:
                        cur.execute(f"""
                                CREATE TABLE {table_name} {schema};""")
                    except:
                        pass
                    for d in data:
                        pk_values = tuple(d[col] for col in primary_key)
                        try:
                            cur.execute(
                                f"SELECT EXISTS (SELECT 1 FROM {table_name} WHERE {' AND '.join([f'{col}=%s' for col in primary_key])})",
                                pk_values)
                            exists = cur.fetchone()[0]
                            if exists:
                                # print(f"Record with primary key {pk_values} already exists, updating...")
                                # Update the record
                                update_cols = [f"{col}=%s" for col in d.keys() if col not in primary_key]
                                update_values = tuple(d[col] for col in d.keys() if col not in primary_key)
                                update_query = f"UPDATE {table_name} SET {', '.join(update_cols)} WHERE {' AND '.join([f'{col}=%s' for col in primary_key])}"
                                cur.execute(update_query, update_values + pk_values)
                            else:
                                # print(f"Record with primary key {pk_values} does not exist, inserting...")
                                # Insert the record
                                insert_cols = list(primary_key) + [col for col in d.keys() if col not in primary_key]
                                insert_values = tuple(d[col] for col in insert_cols)
                                insert_query = f"INSERT INTO {table_name} ({', '.join(insert_cols)}) VALUES ({', '.join(['%s' for _ in insert_cols])})"
                                cur.execute(insert_query, insert_values)
                            conn.commit()
                        except Exception as e:
                            conn.rollback()

                    conn.commit()
                    conn.close()
            else:
                print("Data fetched is empty. So no records written to data base")

        import json
        import argparse
        _parser = argparse.ArgumentParser(prog='Write data', description='')
        _parser.add_argument("--greenplum-details", dest="greenplum_details", type=json.loads, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--table-details", dest="table_details", type=json.loads, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--greenplum-input-data-path", dest="greenplum_input_data_path", type=json.loads, required=False, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = write_data(**_parsed_args)
      image: python:3.11
      imagePullPolicy: IfNotPresent
    inputs:
      parameters:
      - {name: greenplum_details}
      - {name: prepare-keywords-keyword_transformed}
      - {name: table_details_keywords}
    metadata:
      annotations: {pipelines.kubeflow.org/task_display_name: keywords_to_greenplum,
        pipelines.kubeflow.org/component_spec: '{"implementation": {"container": {"args":
          ["--greenplum-details", {"inputValue": "greenplum_details"}, "--table-details",
          {"inputValue": "table_details"}, {"if": {"cond": {"isPresent": "greenplum_input_data_path"},
          "then": ["--greenplum-input-data-path", {"inputValue": "greenplum_input_data_path"}]}}],
          "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip
          install --quiet --no-warn-script-location ''psycopg2'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''psycopg2'' --user)
          && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\"
          > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def write_data(greenplum_details,table_details,greenplum_input_data_path
          = None ):\n    import psycopg2\n    import json\n    from json import load\n    import
          time\n    # load text data\n    print(greenplum_input_data_path)\n    if
          greenplum_input_data_path != None and greenplum_input_data_path != [] :\n        data
          = greenplum_input_data_path\n    elif greenplum_input_data_path == None
          and greenplum_input_data_path != []:\n        print(greenplum_input_data_path)\n        print(\"writing
          last modified time to db\")\n        epoch_ts=int(time.time() * 1000)\n        data
          = [{table_details[\"schema\"].split(\" \")[0][1:]: epoch_ts}]\n    elif
          greenplum_input_data_path == []:\n        data=greenplum_input_data_path\n\n    if
          data != []:\n        # Connect to Greenplum\n        conn = psycopg2.connect(\n            host=greenplum_details[\"host\"],\n            database=greenplum_details[\"database\"],\n            user=greenplum_details[\"user_name\"],\n            password=greenplum_details[\"password\"],\n            port=greenplum_details[\"port\"]\n        )\n        #
          Set autocommit to True to start a new transaction\n        conn.autocommit
          = True\n        cur = conn.cursor()\n\n        mode=table_details[\"mode\"]\n        table_name=table_details[\"table_name\"]\n        schema=table_details[\"schema\"]\n        primary_key=table_details[\"primary_key\"].split('','')\n\n        #
          def bulk_load(data, table_name, schema, cur):\n        if mode == \"bulk_mode\":\n            cur.execute(f\"DROP
          TABLE IF EXISTS {table_name}\")\n            cur.execute(f\"\"\"\n                    CREATE
          TABLE {table_name} {schema};\"\"\")\n\n            for d in data:\n                keys=tuple(d.keys())\n                keys
          = str(keys)\n                value = tuple(d.values())\n                value_format
          = []\n                for cou in range(len(value)):\n                    value_format.append(''%s'')\n\n                keys
          = keys.replace(\"''\", \"\")\n                value_format = str(tuple(value_format))\n                value_format
          = value_format.replace(\"''\", \"\")\n                print(keys,value_format,table_name)\n                if
          len(tuple(d.keys())) == 1 :\n                    keys=keys.replace(\",\",\"\")\n                    value_format=value_format.replace(\",\",\"\")\n                print(keys,
          value_format, table_name)\n                cur.execute(f\"\"\"\n                    INSERT
          INTO {table_name} {keys}\n                    VALUES {value_format};\n                \"\"\",
          value)\n            conn.commit()  # commit the transaction\n            conn.close()  #
          Close the connection\n        else:\n            try:\n                cur.execute(f\"\"\"\n                        CREATE
          TABLE {table_name} {schema};\"\"\")\n            except:\n                pass\n            for
          d in data:\n                pk_values = tuple(d[col] for col in primary_key)\n                try:\n                    cur.execute(\n                        f\"SELECT
          EXISTS (SELECT 1 FROM {table_name} WHERE {'' AND ''.join([f''{col}=%s''
          for col in primary_key])})\",\n                        pk_values)\n                    exists
          = cur.fetchone()[0]\n                    if exists:\n                        #
          print(f\"Record with primary key {pk_values} already exists, updating...\")\n                        #
          Update the record\n                        update_cols = [f\"{col}=%s\"
          for col in d.keys() if col not in primary_key]\n                        update_values
          = tuple(d[col] for col in d.keys() if col not in primary_key)\n                        update_query
          = f\"UPDATE {table_name} SET {'', ''.join(update_cols)} WHERE {'' AND ''.join([f''{col}=%s''
          for col in primary_key])}\"\n                        cur.execute(update_query,
          update_values + pk_values)\n                    else:\n                        #
          print(f\"Record with primary key {pk_values} does not exist, inserting...\")\n                        #
          Insert the record\n                        insert_cols = list(primary_key)
          + [col for col in d.keys() if col not in primary_key]\n                        insert_values
          = tuple(d[col] for col in insert_cols)\n                        insert_query
          = f\"INSERT INTO {table_name} ({'', ''.join(insert_cols)}) VALUES ({'',
          ''.join([''%s'' for _ in insert_cols])})\"\n                        cur.execute(insert_query,
          insert_values)\n                    conn.commit()\n                except
          Exception as e:\n                    conn.rollback()\n\n            conn.commit()\n            conn.close()\n    else:\n        print(\"Data
          fetched is empty. So no records written to data base\")\n\nimport json\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Write data'', description='''')\n_parser.add_argument(\"--greenplum-details\",
          dest=\"greenplum_details\", type=json.loads, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--table-details\",
          dest=\"table_details\", type=json.loads, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--greenplum-input-data-path\",
          dest=\"greenplum_input_data_path\", type=json.loads, required=False, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = write_data(**_parsed_args)\n"],
          "image": "python:3.11"}}, "inputs": [{"name": "greenplum_details", "type":
          "JsonObject"}, {"name": "table_details", "type": "JsonObject"}, {"name":
          "greenplum_input_data_path", "optional": true, "type": "JsonObject"}], "name":
          "Write data"}', pipelines.kubeflow.org/component_ref: '{"digest": "cc5f610af5956a30ae335930d9a85d35002182c9eac1fd670d4278d19f1a16f3",
          "url": "Greenplum/write/component.yaml"}', pipelines.kubeflow.org/arguments.parameters: '{"greenplum_details":
          "{{inputs.parameters.greenplum_details}}", "greenplum_input_data_path":
          "{{inputs.parameters.prepare-keywords-keyword_transformed}}", "table_details":
          "{{inputs.parameters.table_details_keywords}}"}', pipelines.kubeflow.org/max_cache_staleness: P0D}
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.19
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
  - name: write-data-3
    container:
      args: [--greenplum-details, '{{inputs.parameters.greenplum_details}}', --table-details,
        '{{inputs.parameters.table_details_topics}}', --greenplum-input-data-path,
        '{{inputs.parameters.prepare-topics-topic_transformed}}']
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'psycopg2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
        --no-warn-script-location 'psycopg2' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def write_data(greenplum_details,table_details,greenplum_input_data_path = None ):
            import psycopg2
            import json
            from json import load
            import time
            # load text data
            print(greenplum_input_data_path)
            if greenplum_input_data_path != None and greenplum_input_data_path != [] :
                data = greenplum_input_data_path
            elif greenplum_input_data_path == None and greenplum_input_data_path != []:
                print(greenplum_input_data_path)
                print("writing last modified time to db")
                epoch_ts=int(time.time() * 1000)
                data = [{table_details["schema"].split(" ")[0][1:]: epoch_ts}]
            elif greenplum_input_data_path == []:
                data=greenplum_input_data_path

            if data != []:
                # Connect to Greenplum
                conn = psycopg2.connect(
                    host=greenplum_details["host"],
                    database=greenplum_details["database"],
                    user=greenplum_details["user_name"],
                    password=greenplum_details["password"],
                    port=greenplum_details["port"]
                )
                # Set autocommit to True to start a new transaction
                conn.autocommit = True
                cur = conn.cursor()

                mode=table_details["mode"]
                table_name=table_details["table_name"]
                schema=table_details["schema"]
                primary_key=table_details["primary_key"].split(',')

                # def bulk_load(data, table_name, schema, cur):
                if mode == "bulk_mode":
                    cur.execute(f"DROP TABLE IF EXISTS {table_name}")
                    cur.execute(f"""
                            CREATE TABLE {table_name} {schema};""")

                    for d in data:
                        keys=tuple(d.keys())
                        keys = str(keys)
                        value = tuple(d.values())
                        value_format = []
                        for cou in range(len(value)):
                            value_format.append('%s')

                        keys = keys.replace("'", "")
                        value_format = str(tuple(value_format))
                        value_format = value_format.replace("'", "")
                        print(keys,value_format,table_name)
                        if len(tuple(d.keys())) == 1 :
                            keys=keys.replace(",","")
                            value_format=value_format.replace(",","")
                        print(keys, value_format, table_name)
                        cur.execute(f"""
                            INSERT INTO {table_name} {keys}
                            VALUES {value_format};
                        """, value)
                    conn.commit()  # commit the transaction
                    conn.close()  # Close the connection
                else:
                    try:
                        cur.execute(f"""
                                CREATE TABLE {table_name} {schema};""")
                    except:
                        pass
                    for d in data:
                        pk_values = tuple(d[col] for col in primary_key)
                        try:
                            cur.execute(
                                f"SELECT EXISTS (SELECT 1 FROM {table_name} WHERE {' AND '.join([f'{col}=%s' for col in primary_key])})",
                                pk_values)
                            exists = cur.fetchone()[0]
                            if exists:
                                # print(f"Record with primary key {pk_values} already exists, updating...")
                                # Update the record
                                update_cols = [f"{col}=%s" for col in d.keys() if col not in primary_key]
                                update_values = tuple(d[col] for col in d.keys() if col not in primary_key)
                                update_query = f"UPDATE {table_name} SET {', '.join(update_cols)} WHERE {' AND '.join([f'{col}=%s' for col in primary_key])}"
                                cur.execute(update_query, update_values + pk_values)
                            else:
                                # print(f"Record with primary key {pk_values} does not exist, inserting...")
                                # Insert the record
                                insert_cols = list(primary_key) + [col for col in d.keys() if col not in primary_key]
                                insert_values = tuple(d[col] for col in insert_cols)
                                insert_query = f"INSERT INTO {table_name} ({', '.join(insert_cols)}) VALUES ({', '.join(['%s' for _ in insert_cols])})"
                                cur.execute(insert_query, insert_values)
                            conn.commit()
                        except Exception as e:
                            conn.rollback()

                    conn.commit()
                    conn.close()
            else:
                print("Data fetched is empty. So no records written to data base")

        import json
        import argparse
        _parser = argparse.ArgumentParser(prog='Write data', description='')
        _parser.add_argument("--greenplum-details", dest="greenplum_details", type=json.loads, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--table-details", dest="table_details", type=json.loads, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--greenplum-input-data-path", dest="greenplum_input_data_path", type=json.loads, required=False, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = write_data(**_parsed_args)
      image: python:3.11
      imagePullPolicy: IfNotPresent
    inputs:
      parameters:
      - {name: greenplum_details}
      - {name: prepare-topics-topic_transformed}
      - {name: table_details_topics}
    metadata:
      annotations: {pipelines.kubeflow.org/task_display_name: topics_to_greenplum,
        pipelines.kubeflow.org/component_spec: '{"implementation": {"container": {"args":
          ["--greenplum-details", {"inputValue": "greenplum_details"}, "--table-details",
          {"inputValue": "table_details"}, {"if": {"cond": {"isPresent": "greenplum_input_data_path"},
          "then": ["--greenplum-input-data-path", {"inputValue": "greenplum_input_data_path"}]}}],
          "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip
          install --quiet --no-warn-script-location ''psycopg2'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''psycopg2'' --user)
          && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\"
          > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def write_data(greenplum_details,table_details,greenplum_input_data_path
          = None ):\n    import psycopg2\n    import json\n    from json import load\n    import
          time\n    # load text data\n    print(greenplum_input_data_path)\n    if
          greenplum_input_data_path != None and greenplum_input_data_path != [] :\n        data
          = greenplum_input_data_path\n    elif greenplum_input_data_path == None
          and greenplum_input_data_path != []:\n        print(greenplum_input_data_path)\n        print(\"writing
          last modified time to db\")\n        epoch_ts=int(time.time() * 1000)\n        data
          = [{table_details[\"schema\"].split(\" \")[0][1:]: epoch_ts}]\n    elif
          greenplum_input_data_path == []:\n        data=greenplum_input_data_path\n\n    if
          data != []:\n        # Connect to Greenplum\n        conn = psycopg2.connect(\n            host=greenplum_details[\"host\"],\n            database=greenplum_details[\"database\"],\n            user=greenplum_details[\"user_name\"],\n            password=greenplum_details[\"password\"],\n            port=greenplum_details[\"port\"]\n        )\n        #
          Set autocommit to True to start a new transaction\n        conn.autocommit
          = True\n        cur = conn.cursor()\n\n        mode=table_details[\"mode\"]\n        table_name=table_details[\"table_name\"]\n        schema=table_details[\"schema\"]\n        primary_key=table_details[\"primary_key\"].split('','')\n\n        #
          def bulk_load(data, table_name, schema, cur):\n        if mode == \"bulk_mode\":\n            cur.execute(f\"DROP
          TABLE IF EXISTS {table_name}\")\n            cur.execute(f\"\"\"\n                    CREATE
          TABLE {table_name} {schema};\"\"\")\n\n            for d in data:\n                keys=tuple(d.keys())\n                keys
          = str(keys)\n                value = tuple(d.values())\n                value_format
          = []\n                for cou in range(len(value)):\n                    value_format.append(''%s'')\n\n                keys
          = keys.replace(\"''\", \"\")\n                value_format = str(tuple(value_format))\n                value_format
          = value_format.replace(\"''\", \"\")\n                print(keys,value_format,table_name)\n                if
          len(tuple(d.keys())) == 1 :\n                    keys=keys.replace(\",\",\"\")\n                    value_format=value_format.replace(\",\",\"\")\n                print(keys,
          value_format, table_name)\n                cur.execute(f\"\"\"\n                    INSERT
          INTO {table_name} {keys}\n                    VALUES {value_format};\n                \"\"\",
          value)\n            conn.commit()  # commit the transaction\n            conn.close()  #
          Close the connection\n        else:\n            try:\n                cur.execute(f\"\"\"\n                        CREATE
          TABLE {table_name} {schema};\"\"\")\n            except:\n                pass\n            for
          d in data:\n                pk_values = tuple(d[col] for col in primary_key)\n                try:\n                    cur.execute(\n                        f\"SELECT
          EXISTS (SELECT 1 FROM {table_name} WHERE {'' AND ''.join([f''{col}=%s''
          for col in primary_key])})\",\n                        pk_values)\n                    exists
          = cur.fetchone()[0]\n                    if exists:\n                        #
          print(f\"Record with primary key {pk_values} already exists, updating...\")\n                        #
          Update the record\n                        update_cols = [f\"{col}=%s\"
          for col in d.keys() if col not in primary_key]\n                        update_values
          = tuple(d[col] for col in d.keys() if col not in primary_key)\n                        update_query
          = f\"UPDATE {table_name} SET {'', ''.join(update_cols)} WHERE {'' AND ''.join([f''{col}=%s''
          for col in primary_key])}\"\n                        cur.execute(update_query,
          update_values + pk_values)\n                    else:\n                        #
          print(f\"Record with primary key {pk_values} does not exist, inserting...\")\n                        #
          Insert the record\n                        insert_cols = list(primary_key)
          + [col for col in d.keys() if col not in primary_key]\n                        insert_values
          = tuple(d[col] for col in insert_cols)\n                        insert_query
          = f\"INSERT INTO {table_name} ({'', ''.join(insert_cols)}) VALUES ({'',
          ''.join([''%s'' for _ in insert_cols])})\"\n                        cur.execute(insert_query,
          insert_values)\n                    conn.commit()\n                except
          Exception as e:\n                    conn.rollback()\n\n            conn.commit()\n            conn.close()\n    else:\n        print(\"Data
          fetched is empty. So no records written to data base\")\n\nimport json\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Write data'', description='''')\n_parser.add_argument(\"--greenplum-details\",
          dest=\"greenplum_details\", type=json.loads, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--table-details\",
          dest=\"table_details\", type=json.loads, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--greenplum-input-data-path\",
          dest=\"greenplum_input_data_path\", type=json.loads, required=False, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = write_data(**_parsed_args)\n"],
          "image": "python:3.11"}}, "inputs": [{"name": "greenplum_details", "type":
          "JsonObject"}, {"name": "table_details", "type": "JsonObject"}, {"name":
          "greenplum_input_data_path", "optional": true, "type": "JsonObject"}], "name":
          "Write data"}', pipelines.kubeflow.org/component_ref: '{"digest": "cc5f610af5956a30ae335930d9a85d35002182c9eac1fd670d4278d19f1a16f3",
          "url": "Greenplum/write/component.yaml"}', pipelines.kubeflow.org/arguments.parameters: '{"greenplum_details":
          "{{inputs.parameters.greenplum_details}}", "greenplum_input_data_path":
          "{{inputs.parameters.prepare-topics-topic_transformed}}", "table_details":
          "{{inputs.parameters.table_details_topics}}"}', pipelines.kubeflow.org/max_cache_staleness: P0D}
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.19
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
  - name: write-data-4
    container:
      args: [--greenplum-details, '{{inputs.parameters.greenplum_details}}', --table-details,
        '{{inputs.parameters.table_details_sentiment}}', --greenplum-input-data-path,
        '{{inputs.parameters.prepare-sentiment-sentiment_transformed}}']
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'psycopg2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
        --no-warn-script-location 'psycopg2' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def write_data(greenplum_details,table_details,greenplum_input_data_path = None ):
            import psycopg2
            import json
            from json import load
            import time
            # load text data
            print(greenplum_input_data_path)
            if greenplum_input_data_path != None and greenplum_input_data_path != [] :
                data = greenplum_input_data_path
            elif greenplum_input_data_path == None and greenplum_input_data_path != []:
                print(greenplum_input_data_path)
                print("writing last modified time to db")
                epoch_ts=int(time.time() * 1000)
                data = [{table_details["schema"].split(" ")[0][1:]: epoch_ts}]
            elif greenplum_input_data_path == []:
                data=greenplum_input_data_path

            if data != []:
                # Connect to Greenplum
                conn = psycopg2.connect(
                    host=greenplum_details["host"],
                    database=greenplum_details["database"],
                    user=greenplum_details["user_name"],
                    password=greenplum_details["password"],
                    port=greenplum_details["port"]
                )
                # Set autocommit to True to start a new transaction
                conn.autocommit = True
                cur = conn.cursor()

                mode=table_details["mode"]
                table_name=table_details["table_name"]
                schema=table_details["schema"]
                primary_key=table_details["primary_key"].split(',')

                # def bulk_load(data, table_name, schema, cur):
                if mode == "bulk_mode":
                    cur.execute(f"DROP TABLE IF EXISTS {table_name}")
                    cur.execute(f"""
                            CREATE TABLE {table_name} {schema};""")

                    for d in data:
                        keys=tuple(d.keys())
                        keys = str(keys)
                        value = tuple(d.values())
                        value_format = []
                        for cou in range(len(value)):
                            value_format.append('%s')

                        keys = keys.replace("'", "")
                        value_format = str(tuple(value_format))
                        value_format = value_format.replace("'", "")
                        print(keys,value_format,table_name)
                        if len(tuple(d.keys())) == 1 :
                            keys=keys.replace(",","")
                            value_format=value_format.replace(",","")
                        print(keys, value_format, table_name)
                        cur.execute(f"""
                            INSERT INTO {table_name} {keys}
                            VALUES {value_format};
                        """, value)
                    conn.commit()  # commit the transaction
                    conn.close()  # Close the connection
                else:
                    try:
                        cur.execute(f"""
                                CREATE TABLE {table_name} {schema};""")
                    except:
                        pass
                    for d in data:
                        pk_values = tuple(d[col] for col in primary_key)
                        try:
                            cur.execute(
                                f"SELECT EXISTS (SELECT 1 FROM {table_name} WHERE {' AND '.join([f'{col}=%s' for col in primary_key])})",
                                pk_values)
                            exists = cur.fetchone()[0]
                            if exists:
                                # print(f"Record with primary key {pk_values} already exists, updating...")
                                # Update the record
                                update_cols = [f"{col}=%s" for col in d.keys() if col not in primary_key]
                                update_values = tuple(d[col] for col in d.keys() if col not in primary_key)
                                update_query = f"UPDATE {table_name} SET {', '.join(update_cols)} WHERE {' AND '.join([f'{col}=%s' for col in primary_key])}"
                                cur.execute(update_query, update_values + pk_values)
                            else:
                                # print(f"Record with primary key {pk_values} does not exist, inserting...")
                                # Insert the record
                                insert_cols = list(primary_key) + [col for col in d.keys() if col not in primary_key]
                                insert_values = tuple(d[col] for col in insert_cols)
                                insert_query = f"INSERT INTO {table_name} ({', '.join(insert_cols)}) VALUES ({', '.join(['%s' for _ in insert_cols])})"
                                cur.execute(insert_query, insert_values)
                            conn.commit()
                        except Exception as e:
                            conn.rollback()

                    conn.commit()
                    conn.close()
            else:
                print("Data fetched is empty. So no records written to data base")

        import json
        import argparse
        _parser = argparse.ArgumentParser(prog='Write data', description='')
        _parser.add_argument("--greenplum-details", dest="greenplum_details", type=json.loads, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--table-details", dest="table_details", type=json.loads, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--greenplum-input-data-path", dest="greenplum_input_data_path", type=json.loads, required=False, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = write_data(**_parsed_args)
      image: python:3.11
      imagePullPolicy: IfNotPresent
    inputs:
      parameters:
      - {name: greenplum_details}
      - {name: prepare-sentiment-sentiment_transformed}
      - {name: table_details_sentiment}
    metadata:
      annotations: {pipelines.kubeflow.org/task_display_name: sentiment_to_greenplum,
        pipelines.kubeflow.org/component_spec: '{"implementation": {"container": {"args":
          ["--greenplum-details", {"inputValue": "greenplum_details"}, "--table-details",
          {"inputValue": "table_details"}, {"if": {"cond": {"isPresent": "greenplum_input_data_path"},
          "then": ["--greenplum-input-data-path", {"inputValue": "greenplum_input_data_path"}]}}],
          "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip
          install --quiet --no-warn-script-location ''psycopg2'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''psycopg2'' --user)
          && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\"
          > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def write_data(greenplum_details,table_details,greenplum_input_data_path
          = None ):\n    import psycopg2\n    import json\n    from json import load\n    import
          time\n    # load text data\n    print(greenplum_input_data_path)\n    if
          greenplum_input_data_path != None and greenplum_input_data_path != [] :\n        data
          = greenplum_input_data_path\n    elif greenplum_input_data_path == None
          and greenplum_input_data_path != []:\n        print(greenplum_input_data_path)\n        print(\"writing
          last modified time to db\")\n        epoch_ts=int(time.time() * 1000)\n        data
          = [{table_details[\"schema\"].split(\" \")[0][1:]: epoch_ts}]\n    elif
          greenplum_input_data_path == []:\n        data=greenplum_input_data_path\n\n    if
          data != []:\n        # Connect to Greenplum\n        conn = psycopg2.connect(\n            host=greenplum_details[\"host\"],\n            database=greenplum_details[\"database\"],\n            user=greenplum_details[\"user_name\"],\n            password=greenplum_details[\"password\"],\n            port=greenplum_details[\"port\"]\n        )\n        #
          Set autocommit to True to start a new transaction\n        conn.autocommit
          = True\n        cur = conn.cursor()\n\n        mode=table_details[\"mode\"]\n        table_name=table_details[\"table_name\"]\n        schema=table_details[\"schema\"]\n        primary_key=table_details[\"primary_key\"].split('','')\n\n        #
          def bulk_load(data, table_name, schema, cur):\n        if mode == \"bulk_mode\":\n            cur.execute(f\"DROP
          TABLE IF EXISTS {table_name}\")\n            cur.execute(f\"\"\"\n                    CREATE
          TABLE {table_name} {schema};\"\"\")\n\n            for d in data:\n                keys=tuple(d.keys())\n                keys
          = str(keys)\n                value = tuple(d.values())\n                value_format
          = []\n                for cou in range(len(value)):\n                    value_format.append(''%s'')\n\n                keys
          = keys.replace(\"''\", \"\")\n                value_format = str(tuple(value_format))\n                value_format
          = value_format.replace(\"''\", \"\")\n                print(keys,value_format,table_name)\n                if
          len(tuple(d.keys())) == 1 :\n                    keys=keys.replace(\",\",\"\")\n                    value_format=value_format.replace(\",\",\"\")\n                print(keys,
          value_format, table_name)\n                cur.execute(f\"\"\"\n                    INSERT
          INTO {table_name} {keys}\n                    VALUES {value_format};\n                \"\"\",
          value)\n            conn.commit()  # commit the transaction\n            conn.close()  #
          Close the connection\n        else:\n            try:\n                cur.execute(f\"\"\"\n                        CREATE
          TABLE {table_name} {schema};\"\"\")\n            except:\n                pass\n            for
          d in data:\n                pk_values = tuple(d[col] for col in primary_key)\n                try:\n                    cur.execute(\n                        f\"SELECT
          EXISTS (SELECT 1 FROM {table_name} WHERE {'' AND ''.join([f''{col}=%s''
          for col in primary_key])})\",\n                        pk_values)\n                    exists
          = cur.fetchone()[0]\n                    if exists:\n                        #
          print(f\"Record with primary key {pk_values} already exists, updating...\")\n                        #
          Update the record\n                        update_cols = [f\"{col}=%s\"
          for col in d.keys() if col not in primary_key]\n                        update_values
          = tuple(d[col] for col in d.keys() if col not in primary_key)\n                        update_query
          = f\"UPDATE {table_name} SET {'', ''.join(update_cols)} WHERE {'' AND ''.join([f''{col}=%s''
          for col in primary_key])}\"\n                        cur.execute(update_query,
          update_values + pk_values)\n                    else:\n                        #
          print(f\"Record with primary key {pk_values} does not exist, inserting...\")\n                        #
          Insert the record\n                        insert_cols = list(primary_key)
          + [col for col in d.keys() if col not in primary_key]\n                        insert_values
          = tuple(d[col] for col in insert_cols)\n                        insert_query
          = f\"INSERT INTO {table_name} ({'', ''.join(insert_cols)}) VALUES ({'',
          ''.join([''%s'' for _ in insert_cols])})\"\n                        cur.execute(insert_query,
          insert_values)\n                    conn.commit()\n                except
          Exception as e:\n                    conn.rollback()\n\n            conn.commit()\n            conn.close()\n    else:\n        print(\"Data
          fetched is empty. So no records written to data base\")\n\nimport json\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Write data'', description='''')\n_parser.add_argument(\"--greenplum-details\",
          dest=\"greenplum_details\", type=json.loads, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--table-details\",
          dest=\"table_details\", type=json.loads, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--greenplum-input-data-path\",
          dest=\"greenplum_input_data_path\", type=json.loads, required=False, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = write_data(**_parsed_args)\n"],
          "image": "python:3.11"}}, "inputs": [{"name": "greenplum_details", "type":
          "JsonObject"}, {"name": "table_details", "type": "JsonObject"}, {"name":
          "greenplum_input_data_path", "optional": true, "type": "JsonObject"}], "name":
          "Write data"}', pipelines.kubeflow.org/component_ref: '{"digest": "cc5f610af5956a30ae335930d9a85d35002182c9eac1fd670d4278d19f1a16f3",
          "url": "Greenplum/write/component.yaml"}', pipelines.kubeflow.org/arguments.parameters: '{"greenplum_details":
          "{{inputs.parameters.greenplum_details}}", "greenplum_input_data_path":
          "{{inputs.parameters.prepare-sentiment-sentiment_transformed}}", "table_details":
          "{{inputs.parameters.table_details_sentiment}}"}', pipelines.kubeflow.org/max_cache_staleness: P0D}
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.19
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
  - name: write-data-5
    container:
      args: [--greenplum-details, '{{inputs.parameters.greenplum_details}}', --table-details,
        '{{inputs.parameters.table_details_aspect}}', --greenplum-input-data-path,
        '{{inputs.parameters.prepare-aspects-aspect_transformed}}']
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'psycopg2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
        --no-warn-script-location 'psycopg2' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def write_data(greenplum_details,table_details,greenplum_input_data_path = None ):
            import psycopg2
            import json
            from json import load
            import time
            # load text data
            print(greenplum_input_data_path)
            if greenplum_input_data_path != None and greenplum_input_data_path != [] :
                data = greenplum_input_data_path
            elif greenplum_input_data_path == None and greenplum_input_data_path != []:
                print(greenplum_input_data_path)
                print("writing last modified time to db")
                epoch_ts=int(time.time() * 1000)
                data = [{table_details["schema"].split(" ")[0][1:]: epoch_ts}]
            elif greenplum_input_data_path == []:
                data=greenplum_input_data_path

            if data != []:
                # Connect to Greenplum
                conn = psycopg2.connect(
                    host=greenplum_details["host"],
                    database=greenplum_details["database"],
                    user=greenplum_details["user_name"],
                    password=greenplum_details["password"],
                    port=greenplum_details["port"]
                )
                # Set autocommit to True to start a new transaction
                conn.autocommit = True
                cur = conn.cursor()

                mode=table_details["mode"]
                table_name=table_details["table_name"]
                schema=table_details["schema"]
                primary_key=table_details["primary_key"].split(',')

                # def bulk_load(data, table_name, schema, cur):
                if mode == "bulk_mode":
                    cur.execute(f"DROP TABLE IF EXISTS {table_name}")
                    cur.execute(f"""
                            CREATE TABLE {table_name} {schema};""")

                    for d in data:
                        keys=tuple(d.keys())
                        keys = str(keys)
                        value = tuple(d.values())
                        value_format = []
                        for cou in range(len(value)):
                            value_format.append('%s')

                        keys = keys.replace("'", "")
                        value_format = str(tuple(value_format))
                        value_format = value_format.replace("'", "")
                        print(keys,value_format,table_name)
                        if len(tuple(d.keys())) == 1 :
                            keys=keys.replace(",","")
                            value_format=value_format.replace(",","")
                        print(keys, value_format, table_name)
                        cur.execute(f"""
                            INSERT INTO {table_name} {keys}
                            VALUES {value_format};
                        """, value)
                    conn.commit()  # commit the transaction
                    conn.close()  # Close the connection
                else:
                    try:
                        cur.execute(f"""
                                CREATE TABLE {table_name} {schema};""")
                    except:
                        pass
                    for d in data:
                        pk_values = tuple(d[col] for col in primary_key)
                        try:
                            cur.execute(
                                f"SELECT EXISTS (SELECT 1 FROM {table_name} WHERE {' AND '.join([f'{col}=%s' for col in primary_key])})",
                                pk_values)
                            exists = cur.fetchone()[0]
                            if exists:
                                # print(f"Record with primary key {pk_values} already exists, updating...")
                                # Update the record
                                update_cols = [f"{col}=%s" for col in d.keys() if col not in primary_key]
                                update_values = tuple(d[col] for col in d.keys() if col not in primary_key)
                                update_query = f"UPDATE {table_name} SET {', '.join(update_cols)} WHERE {' AND '.join([f'{col}=%s' for col in primary_key])}"
                                cur.execute(update_query, update_values + pk_values)
                            else:
                                # print(f"Record with primary key {pk_values} does not exist, inserting...")
                                # Insert the record
                                insert_cols = list(primary_key) + [col for col in d.keys() if col not in primary_key]
                                insert_values = tuple(d[col] for col in insert_cols)
                                insert_query = f"INSERT INTO {table_name} ({', '.join(insert_cols)}) VALUES ({', '.join(['%s' for _ in insert_cols])})"
                                cur.execute(insert_query, insert_values)
                            conn.commit()
                        except Exception as e:
                            conn.rollback()

                    conn.commit()
                    conn.close()
            else:
                print("Data fetched is empty. So no records written to data base")

        import json
        import argparse
        _parser = argparse.ArgumentParser(prog='Write data', description='')
        _parser.add_argument("--greenplum-details", dest="greenplum_details", type=json.loads, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--table-details", dest="table_details", type=json.loads, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--greenplum-input-data-path", dest="greenplum_input_data_path", type=json.loads, required=False, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = write_data(**_parsed_args)
      image: python:3.11
      imagePullPolicy: IfNotPresent
    inputs:
      parameters:
      - {name: greenplum_details}
      - {name: prepare-aspects-aspect_transformed}
      - {name: table_details_aspect}
    metadata:
      annotations: {pipelines.kubeflow.org/task_display_name: aspect_to_greenplum,
        pipelines.kubeflow.org/component_spec: '{"implementation": {"container": {"args":
          ["--greenplum-details", {"inputValue": "greenplum_details"}, "--table-details",
          {"inputValue": "table_details"}, {"if": {"cond": {"isPresent": "greenplum_input_data_path"},
          "then": ["--greenplum-input-data-path", {"inputValue": "greenplum_input_data_path"}]}}],
          "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip
          install --quiet --no-warn-script-location ''psycopg2'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''psycopg2'' --user)
          && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\"
          > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def write_data(greenplum_details,table_details,greenplum_input_data_path
          = None ):\n    import psycopg2\n    import json\n    from json import load\n    import
          time\n    # load text data\n    print(greenplum_input_data_path)\n    if
          greenplum_input_data_path != None and greenplum_input_data_path != [] :\n        data
          = greenplum_input_data_path\n    elif greenplum_input_data_path == None
          and greenplum_input_data_path != []:\n        print(greenplum_input_data_path)\n        print(\"writing
          last modified time to db\")\n        epoch_ts=int(time.time() * 1000)\n        data
          = [{table_details[\"schema\"].split(\" \")[0][1:]: epoch_ts}]\n    elif
          greenplum_input_data_path == []:\n        data=greenplum_input_data_path\n\n    if
          data != []:\n        # Connect to Greenplum\n        conn = psycopg2.connect(\n            host=greenplum_details[\"host\"],\n            database=greenplum_details[\"database\"],\n            user=greenplum_details[\"user_name\"],\n            password=greenplum_details[\"password\"],\n            port=greenplum_details[\"port\"]\n        )\n        #
          Set autocommit to True to start a new transaction\n        conn.autocommit
          = True\n        cur = conn.cursor()\n\n        mode=table_details[\"mode\"]\n        table_name=table_details[\"table_name\"]\n        schema=table_details[\"schema\"]\n        primary_key=table_details[\"primary_key\"].split('','')\n\n        #
          def bulk_load(data, table_name, schema, cur):\n        if mode == \"bulk_mode\":\n            cur.execute(f\"DROP
          TABLE IF EXISTS {table_name}\")\n            cur.execute(f\"\"\"\n                    CREATE
          TABLE {table_name} {schema};\"\"\")\n\n            for d in data:\n                keys=tuple(d.keys())\n                keys
          = str(keys)\n                value = tuple(d.values())\n                value_format
          = []\n                for cou in range(len(value)):\n                    value_format.append(''%s'')\n\n                keys
          = keys.replace(\"''\", \"\")\n                value_format = str(tuple(value_format))\n                value_format
          = value_format.replace(\"''\", \"\")\n                print(keys,value_format,table_name)\n                if
          len(tuple(d.keys())) == 1 :\n                    keys=keys.replace(\",\",\"\")\n                    value_format=value_format.replace(\",\",\"\")\n                print(keys,
          value_format, table_name)\n                cur.execute(f\"\"\"\n                    INSERT
          INTO {table_name} {keys}\n                    VALUES {value_format};\n                \"\"\",
          value)\n            conn.commit()  # commit the transaction\n            conn.close()  #
          Close the connection\n        else:\n            try:\n                cur.execute(f\"\"\"\n                        CREATE
          TABLE {table_name} {schema};\"\"\")\n            except:\n                pass\n            for
          d in data:\n                pk_values = tuple(d[col] for col in primary_key)\n                try:\n                    cur.execute(\n                        f\"SELECT
          EXISTS (SELECT 1 FROM {table_name} WHERE {'' AND ''.join([f''{col}=%s''
          for col in primary_key])})\",\n                        pk_values)\n                    exists
          = cur.fetchone()[0]\n                    if exists:\n                        #
          print(f\"Record with primary key {pk_values} already exists, updating...\")\n                        #
          Update the record\n                        update_cols = [f\"{col}=%s\"
          for col in d.keys() if col not in primary_key]\n                        update_values
          = tuple(d[col] for col in d.keys() if col not in primary_key)\n                        update_query
          = f\"UPDATE {table_name} SET {'', ''.join(update_cols)} WHERE {'' AND ''.join([f''{col}=%s''
          for col in primary_key])}\"\n                        cur.execute(update_query,
          update_values + pk_values)\n                    else:\n                        #
          print(f\"Record with primary key {pk_values} does not exist, inserting...\")\n                        #
          Insert the record\n                        insert_cols = list(primary_key)
          + [col for col in d.keys() if col not in primary_key]\n                        insert_values
          = tuple(d[col] for col in insert_cols)\n                        insert_query
          = f\"INSERT INTO {table_name} ({'', ''.join(insert_cols)}) VALUES ({'',
          ''.join([''%s'' for _ in insert_cols])})\"\n                        cur.execute(insert_query,
          insert_values)\n                    conn.commit()\n                except
          Exception as e:\n                    conn.rollback()\n\n            conn.commit()\n            conn.close()\n    else:\n        print(\"Data
          fetched is empty. So no records written to data base\")\n\nimport json\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Write data'', description='''')\n_parser.add_argument(\"--greenplum-details\",
          dest=\"greenplum_details\", type=json.loads, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--table-details\",
          dest=\"table_details\", type=json.loads, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--greenplum-input-data-path\",
          dest=\"greenplum_input_data_path\", type=json.loads, required=False, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = write_data(**_parsed_args)\n"],
          "image": "python:3.11"}}, "inputs": [{"name": "greenplum_details", "type":
          "JsonObject"}, {"name": "table_details", "type": "JsonObject"}, {"name":
          "greenplum_input_data_path", "optional": true, "type": "JsonObject"}], "name":
          "Write data"}', pipelines.kubeflow.org/component_ref: '{"digest": "cc5f610af5956a30ae335930d9a85d35002182c9eac1fd670d4278d19f1a16f3",
          "url": "Greenplum/write/component.yaml"}', pipelines.kubeflow.org/arguments.parameters: '{"greenplum_details":
          "{{inputs.parameters.greenplum_details}}", "greenplum_input_data_path":
          "{{inputs.parameters.prepare-aspects-aspect_transformed}}", "table_details":
          "{{inputs.parameters.table_details_aspect}}"}', pipelines.kubeflow.org/max_cache_staleness: P0D}
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.19
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
  - name: write-data-6
    container:
      args: [--greenplum-details, '{{inputs.parameters.greenplum_details}}', --table-details,
        '{{inputs.parameters.table_details_last_modified}}']
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'psycopg2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
        --no-warn-script-location 'psycopg2' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def write_data(greenplum_details,table_details,greenplum_input_data_path = None ):
            import psycopg2
            import json
            from json import load
            import time
            # load text data
            print(greenplum_input_data_path)
            if greenplum_input_data_path != None and greenplum_input_data_path != [] :
                data = greenplum_input_data_path
            elif greenplum_input_data_path == None and greenplum_input_data_path != []:
                print(greenplum_input_data_path)
                print("writing last modified time to db")
                epoch_ts=int(time.time() * 1000)
                data = [{table_details["schema"].split(" ")[0][1:]: epoch_ts}]
            elif greenplum_input_data_path == []:
                data=greenplum_input_data_path

            if data != []:
                # Connect to Greenplum
                conn = psycopg2.connect(
                    host=greenplum_details["host"],
                    database=greenplum_details["database"],
                    user=greenplum_details["user_name"],
                    password=greenplum_details["password"],
                    port=greenplum_details["port"]
                )
                # Set autocommit to True to start a new transaction
                conn.autocommit = True
                cur = conn.cursor()

                mode=table_details["mode"]
                table_name=table_details["table_name"]
                schema=table_details["schema"]
                primary_key=table_details["primary_key"].split(',')

                # def bulk_load(data, table_name, schema, cur):
                if mode == "bulk_mode":
                    cur.execute(f"DROP TABLE IF EXISTS {table_name}")
                    cur.execute(f"""
                            CREATE TABLE {table_name} {schema};""")

                    for d in data:
                        keys=tuple(d.keys())
                        keys = str(keys)
                        value = tuple(d.values())
                        value_format = []
                        for cou in range(len(value)):
                            value_format.append('%s')

                        keys = keys.replace("'", "")
                        value_format = str(tuple(value_format))
                        value_format = value_format.replace("'", "")
                        print(keys,value_format,table_name)
                        if len(tuple(d.keys())) == 1 :
                            keys=keys.replace(",","")
                            value_format=value_format.replace(",","")
                        print(keys, value_format, table_name)
                        cur.execute(f"""
                            INSERT INTO {table_name} {keys}
                            VALUES {value_format};
                        """, value)
                    conn.commit()  # commit the transaction
                    conn.close()  # Close the connection
                else:
                    try:
                        cur.execute(f"""
                                CREATE TABLE {table_name} {schema};""")
                    except:
                        pass
                    for d in data:
                        pk_values = tuple(d[col] for col in primary_key)
                        try:
                            cur.execute(
                                f"SELECT EXISTS (SELECT 1 FROM {table_name} WHERE {' AND '.join([f'{col}=%s' for col in primary_key])})",
                                pk_values)
                            exists = cur.fetchone()[0]
                            if exists:
                                # print(f"Record with primary key {pk_values} already exists, updating...")
                                # Update the record
                                update_cols = [f"{col}=%s" for col in d.keys() if col not in primary_key]
                                update_values = tuple(d[col] for col in d.keys() if col not in primary_key)
                                update_query = f"UPDATE {table_name} SET {', '.join(update_cols)} WHERE {' AND '.join([f'{col}=%s' for col in primary_key])}"
                                cur.execute(update_query, update_values + pk_values)
                            else:
                                # print(f"Record with primary key {pk_values} does not exist, inserting...")
                                # Insert the record
                                insert_cols = list(primary_key) + [col for col in d.keys() if col not in primary_key]
                                insert_values = tuple(d[col] for col in insert_cols)
                                insert_query = f"INSERT INTO {table_name} ({', '.join(insert_cols)}) VALUES ({', '.join(['%s' for _ in insert_cols])})"
                                cur.execute(insert_query, insert_values)
                            conn.commit()
                        except Exception as e:
                            conn.rollback()

                    conn.commit()
                    conn.close()
            else:
                print("Data fetched is empty. So no records written to data base")

        import json
        import argparse
        _parser = argparse.ArgumentParser(prog='Write data', description='')
        _parser.add_argument("--greenplum-details", dest="greenplum_details", type=json.loads, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--table-details", dest="table_details", type=json.loads, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--greenplum-input-data-path", dest="greenplum_input_data_path", type=json.loads, required=False, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = write_data(**_parsed_args)
      image: python:3.11
      imagePullPolicy: IfNotPresent
    inputs:
      parameters:
      - {name: greenplum_details}
      - {name: table_details_last_modified}
    metadata:
      annotations: {pipelines.kubeflow.org/task_display_name: last_modified_time_to_greenplum,
        pipelines.kubeflow.org/component_spec: '{"implementation": {"container": {"args":
          ["--greenplum-details", {"inputValue": "greenplum_details"}, "--table-details",
          {"inputValue": "table_details"}, {"if": {"cond": {"isPresent": "greenplum_input_data_path"},
          "then": ["--greenplum-input-data-path", {"inputValue": "greenplum_input_data_path"}]}}],
          "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip
          install --quiet --no-warn-script-location ''psycopg2'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''psycopg2'' --user)
          && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\"
          > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def write_data(greenplum_details,table_details,greenplum_input_data_path
          = None ):\n    import psycopg2\n    import json\n    from json import load\n    import
          time\n    # load text data\n    print(greenplum_input_data_path)\n    if
          greenplum_input_data_path != None and greenplum_input_data_path != [] :\n        data
          = greenplum_input_data_path\n    elif greenplum_input_data_path == None
          and greenplum_input_data_path != []:\n        print(greenplum_input_data_path)\n        print(\"writing
          last modified time to db\")\n        epoch_ts=int(time.time() * 1000)\n        data
          = [{table_details[\"schema\"].split(\" \")[0][1:]: epoch_ts}]\n    elif
          greenplum_input_data_path == []:\n        data=greenplum_input_data_path\n\n    if
          data != []:\n        # Connect to Greenplum\n        conn = psycopg2.connect(\n            host=greenplum_details[\"host\"],\n            database=greenplum_details[\"database\"],\n            user=greenplum_details[\"user_name\"],\n            password=greenplum_details[\"password\"],\n            port=greenplum_details[\"port\"]\n        )\n        #
          Set autocommit to True to start a new transaction\n        conn.autocommit
          = True\n        cur = conn.cursor()\n\n        mode=table_details[\"mode\"]\n        table_name=table_details[\"table_name\"]\n        schema=table_details[\"schema\"]\n        primary_key=table_details[\"primary_key\"].split('','')\n\n        #
          def bulk_load(data, table_name, schema, cur):\n        if mode == \"bulk_mode\":\n            cur.execute(f\"DROP
          TABLE IF EXISTS {table_name}\")\n            cur.execute(f\"\"\"\n                    CREATE
          TABLE {table_name} {schema};\"\"\")\n\n            for d in data:\n                keys=tuple(d.keys())\n                keys
          = str(keys)\n                value = tuple(d.values())\n                value_format
          = []\n                for cou in range(len(value)):\n                    value_format.append(''%s'')\n\n                keys
          = keys.replace(\"''\", \"\")\n                value_format = str(tuple(value_format))\n                value_format
          = value_format.replace(\"''\", \"\")\n                print(keys,value_format,table_name)\n                if
          len(tuple(d.keys())) == 1 :\n                    keys=keys.replace(\",\",\"\")\n                    value_format=value_format.replace(\",\",\"\")\n                print(keys,
          value_format, table_name)\n                cur.execute(f\"\"\"\n                    INSERT
          INTO {table_name} {keys}\n                    VALUES {value_format};\n                \"\"\",
          value)\n            conn.commit()  # commit the transaction\n            conn.close()  #
          Close the connection\n        else:\n            try:\n                cur.execute(f\"\"\"\n                        CREATE
          TABLE {table_name} {schema};\"\"\")\n            except:\n                pass\n            for
          d in data:\n                pk_values = tuple(d[col] for col in primary_key)\n                try:\n                    cur.execute(\n                        f\"SELECT
          EXISTS (SELECT 1 FROM {table_name} WHERE {'' AND ''.join([f''{col}=%s''
          for col in primary_key])})\",\n                        pk_values)\n                    exists
          = cur.fetchone()[0]\n                    if exists:\n                        #
          print(f\"Record with primary key {pk_values} already exists, updating...\")\n                        #
          Update the record\n                        update_cols = [f\"{col}=%s\"
          for col in d.keys() if col not in primary_key]\n                        update_values
          = tuple(d[col] for col in d.keys() if col not in primary_key)\n                        update_query
          = f\"UPDATE {table_name} SET {'', ''.join(update_cols)} WHERE {'' AND ''.join([f''{col}=%s''
          for col in primary_key])}\"\n                        cur.execute(update_query,
          update_values + pk_values)\n                    else:\n                        #
          print(f\"Record with primary key {pk_values} does not exist, inserting...\")\n                        #
          Insert the record\n                        insert_cols = list(primary_key)
          + [col for col in d.keys() if col not in primary_key]\n                        insert_values
          = tuple(d[col] for col in insert_cols)\n                        insert_query
          = f\"INSERT INTO {table_name} ({'', ''.join(insert_cols)}) VALUES ({'',
          ''.join([''%s'' for _ in insert_cols])})\"\n                        cur.execute(insert_query,
          insert_values)\n                    conn.commit()\n                except
          Exception as e:\n                    conn.rollback()\n\n            conn.commit()\n            conn.close()\n    else:\n        print(\"Data
          fetched is empty. So no records written to data base\")\n\nimport json\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Write data'', description='''')\n_parser.add_argument(\"--greenplum-details\",
          dest=\"greenplum_details\", type=json.loads, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--table-details\",
          dest=\"table_details\", type=json.loads, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--greenplum-input-data-path\",
          dest=\"greenplum_input_data_path\", type=json.loads, required=False, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = write_data(**_parsed_args)\n"],
          "image": "python:3.11"}}, "inputs": [{"name": "greenplum_details", "type":
          "JsonObject"}, {"name": "table_details", "type": "JsonObject"}, {"name":
          "greenplum_input_data_path", "optional": true, "type": "JsonObject"}], "name":
          "Write data"}', pipelines.kubeflow.org/component_ref: '{"digest": "cc5f610af5956a30ae335930d9a85d35002182c9eac1fd670d4278d19f1a16f3",
          "url": "Greenplum/write/component.yaml"}', pipelines.kubeflow.org/arguments.parameters: '{"greenplum_details":
          "{{inputs.parameters.greenplum_details}}", "table_details": "{{inputs.parameters.table_details_last_modified}}"}',
        pipelines.kubeflow.org/max_cache_staleness: P0D}
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.19
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
  arguments:
    parameters:
    - {name: Transcription_query}
    - {name: agg_metric_query}
    - {name: table_details_promotor}
    - {name: greenplum_details}
    - {name: table_details_keywords}
    - {name: table_details_topics}
    - {name: table_details_sentiment}
    - {name: table_details_aspect}
    - {name: table_details_last_modified}
  serviceAccountName: pipeline-runner
  ttlStrategy: {secondsAfterCompletion: 120}
